{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from read_fashion_mnist import FashionMnistLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "data_loader = FashionMnistLoader().get_all_data()\n",
    "\n",
    "X_train_dev, X_test, y_train_dev, y_test = data_loader.standard_split()\n",
    "X_train, X_dev, y_train, y_dev = data_loader.train_split(1/6)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor(), transforms.Normalize([0], [1])])\n",
    "\n",
    "class FashionMnist(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.data = (torch.from_numpy(X).float()/255).reshape(-1, 1, 28, 28).squeeze()\n",
    "        self.target = torch.from_numpy(y).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, tar = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, tar\n",
    "    \n",
    "train_dataset = FashionMnist(X_train, y_train, transform=transform)\n",
    "dev_dataset = FashionMnist(X_dev, y_dev, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "<https://pytorch.org/docs/stable/nn.html#conv2d>\n",
    "\n",
    "![cnn](imgs/LeNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=1568, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(5, 5), stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        #self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(4, 4), stride=1, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.linear1 = nn.Linear(32*7*7, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        #x = self.dropout1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n",
    "# instantiate and print your Net\n",
    "net = CNNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate accuracy before training\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in test_loader:\n",
    "\n",
    "    outputs = net(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "accuracy = 100.0 * correct.item() / total\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 500\n",
    "net.train()\n",
    "\n",
    "def train(n_epochs):\n",
    "    optimizer.train = True\n",
    "    loss_over_time = [] # to track the loss as the network trains\n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass to calculate the parameter gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            # to convert loss into a scalar and add it to running_loss, we use .item()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_i % print_every == print_every//2:    # print every 100 batches (staring from 50)\n",
    "                avg_loss = running_loss/print_every\n",
    "                # record and print the avg loss over the 100 batches\n",
    "                loss_over_time.append(avg_loss)\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, avg_loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    return loss_over_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 251, Avg. Loss: 0.4702885763645172\n",
      "Epoch: 1, Batch: 751, Avg. Loss: 0.49852801774442196\n",
      "Epoch: 1, Batch: 1251, Avg. Loss: 0.4123904131054878\n",
      "Epoch: 1, Batch: 1751, Avg. Loss: 0.3856947017759085\n",
      "Epoch: 1, Batch: 2251, Avg. Loss: 0.39184120567142966\n",
      "Epoch: 1, Batch: 2751, Avg. Loss: 0.40330247558653354\n",
      "Epoch: 2, Batch: 251, Avg. Loss: 0.17076355582475664\n",
      "Epoch: 2, Batch: 751, Avg. Loss: 0.38064597940444944\n",
      "Epoch: 2, Batch: 1251, Avg. Loss: 0.36074430897831916\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2 # start small to see if your model works, initially\n",
    "\n",
    "# call train and record the loss over time\n",
    "training_loss = train(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot([{\"x\": list(range(len(training_loss))), \"y\": training_loss}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tensor and lists to monitor test loss and accuracy\n",
    "test_loss = torch.zeros(1)\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# set the module to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "for batch_i, data in enumerate(test_loader):\n",
    "    \n",
    "    # get the input images and their corresponding labels\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # forward pass to get outputs\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "            \n",
    "    # update average test loss \n",
    "    test_loss = test_loss + ((torch.ones(1) / (batch_i + 1)) * (loss.data - test_loss))\n",
    "    \n",
    "    # get the predicted class from the maximum value in the output-list of class scores\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    # this creates a `correct` Tensor that holds the number of correctly classified images in a batch\n",
    "    correct = np.squeeze(predicted.eq(labels.data.view_as(predicted)))\n",
    "    \n",
    "    # calculate test accuracy for *each* object class\n",
    "    # we get the scalar value of correct items for a class, by calling `correct[i].item()`\n",
    "    for i in range(batch_size):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss.numpy()[0]))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "        \n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "# get predictions\n",
    "preds = np.squeeze(net(images).data.max(1, keepdim=True)[1].numpy())\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <http://www.matthewzeiler.com/wp-content/uploads/2017/07/eccv2014.pdf>\n",
    "\n",
    "* <https://www.youtube.com/watch?v=ghEmQSxT6tw>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights in the first conv layer\n",
    "weights = net.conv1.weight.data\n",
    "w = weights.numpy()\n",
    "\n",
    "fig=plt.figure(figsize=(16, 16))\n",
    "columns = 4\n",
    "rows = 4\n",
    "for i in range(0, columns*rows):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(w[i][0], cmap='gray')\n",
    "    \n",
    "print('First convolutional layer')\n",
    "plt.show()\n",
    "\n",
    "weights = net.conv2.weight.data\n",
    "w = weights.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of testing images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# select an image by index\n",
    "idx = 1\n",
    "img = np.squeeze(images[idx])\n",
    "\n",
    "import cv2\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "weights = net.conv1.weight.data\n",
    "w = weights.numpy()\n",
    "\n",
    "# 1. first conv layer\n",
    "# for 10 filters\n",
    "fig=plt.figure(figsize=(30, 10))\n",
    "columns = 4*2\n",
    "rows = 4\n",
    "for i in range(0, columns*rows):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    if ((i%2)==0):\n",
    "        plt.imshow(w[int(i/2)][0], cmap='gray')\n",
    "    else:\n",
    "        c = cv2.filter2D(img, -1, w[int((i-1)/2)][0])\n",
    "        plt.imshow(c, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same process but for the second conv layer (20, 3x3 filters):\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "# second conv layer, conv2\n",
    "weights = net.conv2.weight.data\n",
    "w = weights.numpy()\n",
    "\n",
    "# 1. first conv layer\n",
    "# for 20 filters\n",
    "fig=plt.figure(figsize=(30, 10))\n",
    "columns = 8*2\n",
    "rows = 4\n",
    "for i in range(0, columns*rows):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    if ((i%2)==0):\n",
    "        plt.imshow(w[int(i/2)][0], cmap='gray')\n",
    "    else:\n",
    "        c = cv2.filter2D(img, -1, w[int((i-1)/2)][0])\n",
    "        plt.imshow(c, cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
