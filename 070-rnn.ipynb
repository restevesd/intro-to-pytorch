{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "![LSTM](imgs/LSTM3-chain.png)\n",
    "\n",
    "![LSTM](imgs/LSTM2-notation.png)\n",
    "\n",
    "\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-f.png)\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-i.png)\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-C.png)\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-o.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From pytorch documentation\n",
    "\n",
    "\\begin{array}{ll} \\\\\n",
    "    f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "    i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "    g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "    c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
    "    o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\    \n",
    "    h_t = o_t * \\tanh(c_t) \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "input_size = 3\n",
    "hidden_size = 4 \n",
    "\n",
    "inputs = torch.randn(seq_len, batch_size, input_size)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hidden_0 = (h_0, c_0)\n",
    "hidden_0 = (torch.zeros(1, batch_size, hidden_size), torch.zeros(1, batch_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out, lstm_hidden = lstm(inputs)\n",
    "lstm_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0299,  0.1668,  0.0243,  0.0144],\n",
       "         [-0.0768,  0.0104, -0.0325, -0.1756]],\n",
       "\n",
       "        [[-0.0837,  0.1039, -0.0421, -0.1588],\n",
       "         [-0.1035,  0.1413, -0.0930, -0.1513]],\n",
       "\n",
       "        [[-0.0694, -0.0477,  0.0200, -0.0866],\n",
       "         [-0.1063,  0.1324, -0.0662, -0.1797]],\n",
       "\n",
       "        [[ 0.0042,  0.1869, -0.0824, -0.0120],\n",
       "         [-0.1908,  0.3426,  0.0453, -0.0387]],\n",
       "\n",
       "        [[-0.0380,  0.1472, -0.1476, -0.1807],\n",
       "         [-0.0675,  0.0157,  0.0231, -0.1499]],\n",
       "\n",
       "        [[-0.0297,  0.1242, -0.1958, -0.3778],\n",
       "         [-0.0506, -0.0345, -0.0547, -0.2500]],\n",
       "\n",
       "        [[-0.0675,  0.2927, -0.0151, -0.0446],\n",
       "         [-0.0991,  0.0627,  0.0881, -0.0023]],\n",
       "\n",
       "        [[ 0.0313,  0.4282, -0.0358, -0.0304],\n",
       "         [-0.1195,  0.1341,  0.0754, -0.0027]],\n",
       "\n",
       "        [[-0.1459,  0.1669,  0.0332, -0.0562],\n",
       "         [-0.1035,  0.0443,  0.0395, -0.0555]],\n",
       "\n",
       "        [[-0.1492,  0.1285,  0.0549, -0.0889],\n",
       "         [-0.1346,  0.1317, -0.0328, -0.1172]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to put hidden inputs to zeros, there is no need to provide them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0299,  0.1668,  0.0243,  0.0144],\n",
       "         [-0.0768,  0.0104, -0.0325, -0.1756]],\n",
       "\n",
       "        [[-0.0837,  0.1039, -0.0421, -0.1588],\n",
       "         [-0.1035,  0.1413, -0.0930, -0.1513]],\n",
       "\n",
       "        [[-0.0694, -0.0477,  0.0200, -0.0866],\n",
       "         [-0.1063,  0.1324, -0.0662, -0.1797]],\n",
       "\n",
       "        [[ 0.0042,  0.1869, -0.0824, -0.0120],\n",
       "         [-0.1908,  0.3426,  0.0453, -0.0387]],\n",
       "\n",
       "        [[-0.0380,  0.1472, -0.1476, -0.1807],\n",
       "         [-0.0675,  0.0157,  0.0231, -0.1499]],\n",
       "\n",
       "        [[-0.0297,  0.1242, -0.1958, -0.3778],\n",
       "         [-0.0506, -0.0345, -0.0547, -0.2500]],\n",
       "\n",
       "        [[-0.0675,  0.2927, -0.0151, -0.0446],\n",
       "         [-0.0991,  0.0627,  0.0881, -0.0023]],\n",
       "\n",
       "        [[ 0.0313,  0.4282, -0.0358, -0.0304],\n",
       "         [-0.1195,  0.1341,  0.0754, -0.0027]],\n",
       "\n",
       "        [[-0.1459,  0.1669,  0.0332, -0.0562],\n",
       "         [-0.1035,  0.0443,  0.0395, -0.0555]],\n",
       "\n",
       "        [[-0.1492,  0.1285,  0.0549, -0.0889],\n",
       "         [-0.1346,  0.1317, -0.0328, -0.1172]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, lstm_hidden = lstm(inputs)\n",
    "lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the last output is the output of RRR. We can get it by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1492,  0.1285,  0.0549, -0.0889],\n",
       "        [-0.1346,  0.1317, -0.0328, -0.1172]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often convient to have batches as the first dimension of the input. One can do it by adding `batch_first=True` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5353,  0.2963,  0.1498],\n",
       "         [-0.3241,  0.7809,  0.1792],\n",
       "         [ 1.0528, -0.5501,  0.3137],\n",
       "         [ 1.0422, -1.1246,  0.5430],\n",
       "         [ 1.4121, -0.9867,  0.1293],\n",
       "         [-0.2408,  2.3817, -0.1098],\n",
       "         [ 0.1807,  0.3314, -1.0077],\n",
       "         [ 0.1203,  0.0134, -0.1932],\n",
       "         [ 0.9413,  0.7572, -1.1836],\n",
       "         [-0.0453,  0.0241, -0.6344]],\n",
       "\n",
       "        [[ 1.7979,  0.3554,  1.3758],\n",
       "         [-0.4562,  0.5411,  0.0063],\n",
       "         [-0.7040,  0.2585,  0.1255],\n",
       "         [ 0.1649, -1.6586, -0.6749],\n",
       "         [ 2.0979, -0.5523, -0.1197],\n",
       "         [-0.4566, -1.6408,  0.6930],\n",
       "         [ 0.7879, -0.1795,  0.4447],\n",
       "         [-1.6472, -0.9431, -0.3560],\n",
       "         [ 0.3737,  0.1944, -1.1263],\n",
       "         [-1.0246, -0.7168,  0.3650]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_batch_first = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True) \n",
    "inputs_batch_first = torch.randn(batch_size, seq_len, input_size)\n",
    "inputs_batch_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0915,  0.0467, -0.0404, -0.0004],\n",
       "         [ 0.0635,  0.0561, -0.0769, -0.0476],\n",
       "         [-0.0006,  0.0280,  0.1081,  0.1938],\n",
       "         [ 0.0631,  0.0140,  0.1391,  0.2828],\n",
       "         [-0.0103, -0.0083,  0.1382,  0.2786],\n",
       "         [-0.0672,  0.0307,  0.0507, -0.0795],\n",
       "         [-0.1571,  0.0734,  0.0449, -0.0384],\n",
       "         [-0.0577,  0.0881,  0.0481,  0.0914],\n",
       "         [-0.2330,  0.0529,  0.0840,  0.0556],\n",
       "         [-0.1159,  0.0950,  0.0578,  0.0899]],\n",
       "\n",
       "        [[-0.0219,  0.0018,  0.1494,  0.1525],\n",
       "         [ 0.0556,  0.0502,  0.0301,  0.0377],\n",
       "         [ 0.1186,  0.0772, -0.0304,  0.0076],\n",
       "         [ 0.0869,  0.0407,  0.0714,  0.2884],\n",
       "         [-0.1217, -0.0095,  0.1357,  0.2347],\n",
       "         [ 0.1495,  0.0206,  0.1275,  0.3637],\n",
       "         [ 0.0788, -0.0014,  0.1637,  0.2802],\n",
       "         [ 0.2336,  0.0570,  0.0615,  0.1754],\n",
       "         [-0.0523,  0.0070,  0.0933,  0.0990],\n",
       "         [ 0.1646,  0.0661,  0.0414,  0.1819]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, lstm_hidden = lstm_batch_first(inputs_batch_first)\n",
    "lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get the finial output by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1159,  0.0950,  0.0578,  0.0899],\n",
       "        [ 0.1646,  0.0661,  0.0414,  0.1819]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  5, 76, 49,  8, 49, 60, 83, 41, 29],\n",
       "        [19,  3, 85,  0, 50, 14, 68, 47,  3,  1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size = 100\n",
    "sentences = torch.randint(dict_size, (batch_size, seq_len))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 3\n",
    "embedding = nn.Embedding(dict_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2196,  0.6031, -0.4558],\n",
       "         [ 1.0393, -0.0342, -0.5091],\n",
       "         [ 1.8944, -0.5545,  1.0928],\n",
       "         [ 0.4852,  0.7222, -0.0678],\n",
       "         [ 0.5222, -0.6337, -1.0984],\n",
       "         [ 0.4852,  0.7222, -0.0678],\n",
       "         [-1.0099, -1.2522,  1.1252],\n",
       "         [-0.8211,  0.3858,  0.6533],\n",
       "         [ 0.2912,  0.3577,  0.1148],\n",
       "         [ 1.5376, -0.3341, -0.6996]],\n",
       "\n",
       "        [[ 1.2343,  0.6949, -2.0666],\n",
       "         [ 0.1641,  0.3015, -0.6134],\n",
       "         [ 0.4257, -0.7820, -0.3327],\n",
       "         [-0.7738, -0.1896,  0.4735],\n",
       "         [ 1.3434, -0.0107,  1.1181],\n",
       "         [-0.1866,  2.2336, -0.3407],\n",
       "         [ 1.0563, -0.2882,  1.3992],\n",
       "         [-0.1661,  0.7801,  0.1108],\n",
       "         [ 0.1641,  0.3015, -0.6134],\n",
       "         [ 1.3613,  0.9567, -0.0597]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_embedded = embedding(sentences)\n",
    "sentences_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1609, -0.0006,  0.1093,  0.2144],\n",
       "        [-0.1601,  0.0300,  0.1046,  0.0955]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, _ = lstm_batch_first(sentences_embedded)\n",
    "lstm_out[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)\n",
    "\n",
    "Next we consider a dataset with text and the goal is to evaluate whether they are toxic or non-toxic.\n",
    "\n",
    "You can download the dataset in the following link:\n",
    "\n",
    "[here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments_df = pd.read_csv(\"data/jigsaw-toxic-comment-classification-challenge/train.csv\")[:10000]\n",
    "comments_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>your support for Chris Lawson \\n\\nAre you sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5578</th>\n",
       "      <td>Apologies for the above litany of bot-generate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text\n",
       "1911  your support for Chris Lawson \\n\\nAre you sure...\n",
       "5578  Apologies for the above litany of bot-generate..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "label_colnames = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(comments_df[['comment_text']], comments_df[label_colnames], random_state=667)\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z ]')\n",
    "STEMMER = SnowballStemmer('english')\n",
    "\n",
    "class TextPreprocessor:\n",
    "        \n",
    "    def transfrom_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(BAD_SYMBOLS_RE, \" \", text) # process bad symbols\n",
    "        # text = \" \".join([STEMMER.stem(word) for word in text.split()])\n",
    "        return text\n",
    "    \n",
    "    def transform(self, series):\n",
    "        return series.apply(lambda text: self.transfrom_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "X_train_preprocessed = preprocessor.transform(X_train['comment_text'])\n",
    "X_test_preprocessed = preprocessor.transform(X_test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your support for Chris Lawson \n",
      "\n",
      "Are you sure you want to support this guy?  Look at his source/quote for making the Red Baron jewish.  read the discussion page  on it. See the opposers views on his request page.  It scares me.   JohnHistory\n",
      "\n",
      "http://en.wikipedia.org/wiki/Talk:Manfred_von_Richthofen\n",
      "\n",
      "http://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Clawson\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "your support for chris lawson   are you sure you want to support this guy   look at his source quote for making the red baron jewish   read the discussion page  on it  see the opposers views on his request page   it scares me    johnhistory  http   en wikipedia org wiki talk manfred von richthofen  http   en wikipedia org wiki wikipedia requests for adminship clawson\n"
     ]
    }
   ],
   "source": [
    "print(X_train[\"comment_text\"].iloc[0])\n",
    "print('---------------------------------------------------------------------------------------------------------------------')\n",
    "print(X_train_preprocessed.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apologies for the above litany of bot-generated warnings and threats.  I think the bot needs its rules adjusted, and have notified the operator accordingly.   (call me Russ)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "apologies for the above litany of bot generated warnings and threats   i think the bot needs its rules adjusted  and have notified the operator accordingly     call me russ \n"
     ]
    }
   ],
   "source": [
    "print(X_train[\"comment_text\"].iloc[1])\n",
    "print('---------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "print(X_train_preprocessed.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(text):\n",
    "    word_set = set()\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "    word_list = [\"<UNK>\", \"<PAD>\"] + sorted(list(word_set))\n",
    "    word2idx = {word_list[idx]: idx for idx in range(len(word_list))}\n",
    "    idx2word = {idx: word_list[idx] for idx in range(len(word_list))}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = None\n",
    "        self.idx2word = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        text = \" \".join(X)\n",
    "        self.word2idx, self.idx2word = create_dicts(text)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [self.transform_line(line) for line in X]\n",
    "        \n",
    "    def transform_line(self, line):\n",
    "        return [self.word2idx.get(word, 0) for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(X_train_preprocessed)\n",
    "tokenizer.fit(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenizer.transform(X_train_preprocessed)\n",
    "X_test_tokenized = tokenizer.transform(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutter:\n",
    "\n",
    "    def __init__(self, size=150):\n",
    "        self.size = size\n",
    "        \n",
    "    def transform(self, X):\n",
    "        new_X = []\n",
    "        for line in X:\n",
    "            new_line = line[:self.size]\n",
    "            new_line = new_line + [1] * (self.size - len(new_line))\n",
    "            new_X.append(new_line)\n",
    "        return new_X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = Cutter()\n",
    "X_train_cutted = cutter.transform(X_train_tokenized)\n",
    "X_test_cutted = cutter.transform(X_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.from_numpy(y_train.values)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.tensor(X_train_cutted), torch.from_numpy(y_train.values).float())\n",
    "test_data = TensorDataset(torch.tensor(X_test_cutted), torch.from_numpy(y_test.values).float())\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, dict_size, output_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(dict_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeded)\n",
    "        lstm_out = lstm_out[:, -1]        \n",
    "        logits = self.fc(lstm_out)\n",
    "        out = self.sigmoid(logits)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(tokenizer.word2idx)\n",
    "output_size = len(label_colnames)\n",
    "embedding_dim = 3\n",
    "hidden_dim = 4\n",
    "\n",
    "lstm_model = LSTMModel(dict_size, output_size, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7500, 150])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch = torch.tensor(X_train_cutted)\n",
    "X_test_torch = torch.tensor(X_test_cutted)\n",
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        ...,\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6003, 0.5694, 0.4065, 0.4315, 0.6406, 0.3732],\n",
       "        [0.6229, 0.5699, 0.3899, 0.4838, 0.6790, 0.4039]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model(X_train_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6592, 0.5199, 0.3791, 0.4360, 0.6723, 0.3672],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6073, 0.5258, 0.4068, 0.4471, 0.6408, 0.3721],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.5846, 0.5823, 0.4250, 0.4277, 0.6265, 0.3837],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671],\n",
       "        [0.6593, 0.5196, 0.3791, 0.4358, 0.6721, 0.3671]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "input_data, labels = dataiter.next()\n",
    "lstm_model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1, Avg. Loss: 0.18356561660766602\n",
      "Epoch: 1, Batch: 5, Avg. Loss: 0.6924098134040833\n",
      "Epoch: 1, Batch: 9, Avg. Loss: 0.6495579928159714\n",
      "Epoch: 1, Batch: 13, Avg. Loss: 0.6164238601922989\n",
      "Epoch: 1, Batch: 17, Avg. Loss: 0.5909076780080795\n",
      "Epoch: 1, Batch: 21, Avg. Loss: 0.5543299615383148\n",
      "Epoch: 1, Batch: 25, Avg. Loss: 0.5298996269702911\n",
      "Epoch: 1, Batch: 29, Avg. Loss: 0.49975817650556564\n",
      "Epoch: 1, Batch: 33, Avg. Loss: 0.4733560010790825\n",
      "Epoch: 1, Batch: 37, Avg. Loss: 0.44770464301109314\n",
      "Epoch: 1, Batch: 41, Avg. Loss: 0.42345256358385086\n",
      "Epoch: 1, Batch: 45, Avg. Loss: 0.39359068870544434\n",
      "Epoch: 1, Batch: 49, Avg. Loss: 0.37811052799224854\n",
      "Epoch: 1, Batch: 53, Avg. Loss: 0.3656677082180977\n",
      "Epoch: 1, Batch: 57, Avg. Loss: 0.34512636065483093\n",
      "Epoch: 1, Batch: 61, Avg. Loss: 0.3324815258383751\n",
      "Epoch: 1, Batch: 65, Avg. Loss: 0.28432659804821014\n",
      "Epoch: 1, Batch: 69, Avg. Loss: 0.2632811777293682\n",
      "Epoch: 1, Batch: 73, Avg. Loss: 0.2789395600557327\n",
      "Epoch: 1, Batch: 77, Avg. Loss: 0.25612088292837143\n",
      "Epoch: 1, Batch: 81, Avg. Loss: 0.2394522875547409\n",
      "Epoch: 1, Batch: 85, Avg. Loss: 0.24424131214618683\n",
      "Epoch: 1, Batch: 89, Avg. Loss: 0.23990032821893692\n",
      "Epoch: 1, Batch: 93, Avg. Loss: 0.23087933287024498\n",
      "Epoch: 1, Batch: 97, Avg. Loss: 0.19318589568138123\n",
      "Epoch: 1, Batch: 101, Avg. Loss: 0.18428241088986397\n",
      "Epoch: 1, Batch: 105, Avg. Loss: 0.19515131041407585\n",
      "Epoch: 1, Batch: 109, Avg. Loss: 0.18115411698818207\n",
      "Epoch: 1, Batch: 113, Avg. Loss: 0.18260806053876877\n",
      "Epoch: 1, Batch: 117, Avg. Loss: 0.1867871955037117\n",
      "Epoch: 1, Batch: 121, Avg. Loss: 0.16626939177513123\n",
      "Epoch: 1, Batch: 125, Avg. Loss: 0.2275954745709896\n",
      "Epoch: 1, Batch: 129, Avg. Loss: 0.1706547848880291\n",
      "Epoch: 1, Batch: 133, Avg. Loss: 0.19929538667201996\n",
      "Epoch: 1, Batch: 137, Avg. Loss: 0.18660894595086575\n",
      "Epoch: 1, Batch: 141, Avg. Loss: 0.19833217561244965\n",
      "Epoch: 1, Batch: 145, Avg. Loss: 0.18000171706080437\n",
      "Epoch: 1, Batch: 149, Avg. Loss: 0.1904185153543949\n",
      "Epoch: 1, Batch: 153, Avg. Loss: 0.15234553068876266\n",
      "Epoch: 1, Batch: 157, Avg. Loss: 0.09453088417649269\n",
      "Epoch: 1, Batch: 161, Avg. Loss: 0.12523768655955791\n",
      "Epoch: 1, Batch: 165, Avg. Loss: 0.12016060389578342\n",
      "Epoch: 1, Batch: 169, Avg. Loss: 0.1616298258304596\n",
      "Epoch: 1, Batch: 173, Avg. Loss: 0.14741580188274384\n",
      "Epoch: 1, Batch: 177, Avg. Loss: 0.13839556835591793\n",
      "Epoch: 1, Batch: 181, Avg. Loss: 0.135909516364336\n",
      "Epoch: 1, Batch: 185, Avg. Loss: 0.13344385288655758\n",
      "Epoch: 1, Batch: 189, Avg. Loss: 0.17312097176909447\n",
      "Epoch: 1, Batch: 193, Avg. Loss: 0.1567717306315899\n",
      "Epoch: 1, Batch: 197, Avg. Loss: 0.15401039272546768\n",
      "Epoch: 1, Batch: 201, Avg. Loss: 0.10211203526705503\n",
      "Epoch: 1, Batch: 205, Avg. Loss: 0.12858791463077068\n",
      "Epoch: 1, Batch: 209, Avg. Loss: 0.11528503149747849\n",
      "Epoch: 1, Batch: 213, Avg. Loss: 0.13357154093682766\n",
      "Epoch: 1, Batch: 217, Avg. Loss: 0.1341459173709154\n",
      "Epoch: 1, Batch: 221, Avg. Loss: 0.21122510731220245\n",
      "Epoch: 1, Batch: 225, Avg. Loss: 0.17059425078332424\n",
      "Epoch: 1, Batch: 229, Avg. Loss: 0.15105549804866314\n",
      "Epoch: 1, Batch: 233, Avg. Loss: 0.16409260407090187\n",
      "Epoch: 2, Batch: 1, Avg. Loss: 0.039789196103811264\n",
      "Epoch: 2, Batch: 5, Avg. Loss: 0.09866199549287558\n",
      "Epoch: 2, Batch: 9, Avg. Loss: 0.1607114914804697\n",
      "Epoch: 2, Batch: 13, Avg. Loss: 0.14685310050845146\n",
      "Epoch: 2, Batch: 17, Avg. Loss: 0.16667752526700497\n",
      "Epoch: 2, Batch: 21, Avg. Loss: 0.1967573706060648\n",
      "Epoch: 2, Batch: 25, Avg. Loss: 0.13997391983866692\n",
      "Epoch: 2, Batch: 29, Avg. Loss: 0.19436052441596985\n",
      "Epoch: 2, Batch: 33, Avg. Loss: 0.16250602528452873\n",
      "Epoch: 2, Batch: 37, Avg. Loss: 0.09941177163273096\n",
      "Epoch: 2, Batch: 41, Avg. Loss: 0.13040970172733068\n",
      "Epoch: 2, Batch: 45, Avg. Loss: 0.09585468471050262\n",
      "Epoch: 2, Batch: 49, Avg. Loss: 0.12909855227917433\n",
      "Epoch: 2, Batch: 53, Avg. Loss: 0.1710360050201416\n",
      "Epoch: 2, Batch: 57, Avg. Loss: 0.12463757023215294\n",
      "Epoch: 2, Batch: 61, Avg. Loss: 0.11666948813945055\n",
      "Epoch: 2, Batch: 65, Avg. Loss: 0.14405758678913116\n",
      "Epoch: 2, Batch: 69, Avg. Loss: 0.17804139852523804\n",
      "Epoch: 2, Batch: 73, Avg. Loss: 0.156579514965415\n",
      "Epoch: 2, Batch: 77, Avg. Loss: 0.11511828750371933\n",
      "Epoch: 2, Batch: 81, Avg. Loss: 0.1752883680164814\n",
      "Epoch: 2, Batch: 85, Avg. Loss: 0.1476264763623476\n",
      "Epoch: 2, Batch: 89, Avg. Loss: 0.10547149274498224\n",
      "Epoch: 2, Batch: 93, Avg. Loss: 0.16471503302454948\n",
      "Epoch: 2, Batch: 97, Avg. Loss: 0.12571122869849205\n",
      "Epoch: 2, Batch: 101, Avg. Loss: 0.1882793214172125\n",
      "Epoch: 2, Batch: 105, Avg. Loss: 0.17380782589316368\n",
      "Epoch: 2, Batch: 109, Avg. Loss: 0.12671493738889694\n",
      "Epoch: 2, Batch: 113, Avg. Loss: 0.16518783569335938\n",
      "Epoch: 2, Batch: 117, Avg. Loss: 0.14029606617987156\n",
      "Epoch: 2, Batch: 121, Avg. Loss: 0.14164089038968086\n",
      "Epoch: 2, Batch: 125, Avg. Loss: 0.12743965536355972\n",
      "Epoch: 2, Batch: 129, Avg. Loss: 0.16456553153693676\n",
      "Epoch: 2, Batch: 133, Avg. Loss: 0.1126638064160943\n",
      "Epoch: 2, Batch: 137, Avg. Loss: 0.12581801414489746\n",
      "Epoch: 2, Batch: 141, Avg. Loss: 0.16972741670906544\n",
      "Epoch: 2, Batch: 145, Avg. Loss: 0.14406614191830158\n",
      "Epoch: 2, Batch: 149, Avg. Loss: 0.12087246775627136\n",
      "Epoch: 2, Batch: 153, Avg. Loss: 0.16519521921873093\n",
      "Epoch: 2, Batch: 157, Avg. Loss: 0.1679068934172392\n",
      "Epoch: 2, Batch: 161, Avg. Loss: 0.1532843206077814\n",
      "Epoch: 2, Batch: 165, Avg. Loss: 0.13335854560136795\n",
      "Epoch: 2, Batch: 169, Avg. Loss: 0.10423645935952663\n",
      "Epoch: 2, Batch: 173, Avg. Loss: 0.13365990109741688\n",
      "Epoch: 2, Batch: 177, Avg. Loss: 0.13705031014978886\n",
      "Epoch: 2, Batch: 181, Avg. Loss: 0.12249252572655678\n",
      "Epoch: 2, Batch: 185, Avg. Loss: 0.13127516582608223\n",
      "Epoch: 2, Batch: 189, Avg. Loss: 0.16368239372968674\n",
      "Epoch: 2, Batch: 193, Avg. Loss: 0.11951762903481722\n",
      "Epoch: 2, Batch: 197, Avg. Loss: 0.14349201042205095\n",
      "Epoch: 2, Batch: 201, Avg. Loss: 0.09031555149704218\n",
      "Epoch: 2, Batch: 205, Avg. Loss: 0.1522806230932474\n",
      "Epoch: 2, Batch: 209, Avg. Loss: 0.12200258299708366\n",
      "Epoch: 2, Batch: 213, Avg. Loss: 0.13315093331038952\n",
      "Epoch: 2, Batch: 217, Avg. Loss: 0.17815490812063217\n",
      "Epoch: 2, Batch: 221, Avg. Loss: 0.1540581863373518\n",
      "Epoch: 2, Batch: 225, Avg. Loss: 0.1211383007466793\n",
      "Epoch: 2, Batch: 229, Avg. Loss: 0.0981864221394062\n",
      "Epoch: 2, Batch: 233, Avg. Loss: 0.1889762170612812\n",
      "Epoch: 3, Batch: 1, Avg. Loss: 0.05246241018176079\n",
      "Epoch: 3, Batch: 5, Avg. Loss: 0.18168146908283234\n",
      "Epoch: 3, Batch: 9, Avg. Loss: 0.10328999441117048\n",
      "Epoch: 3, Batch: 13, Avg. Loss: 0.1653453279286623\n",
      "Epoch: 3, Batch: 17, Avg. Loss: 0.12833227589726448\n",
      "Epoch: 3, Batch: 21, Avg. Loss: 0.136391656473279\n",
      "Epoch: 3, Batch: 25, Avg. Loss: 0.13727309741079807\n",
      "Epoch: 3, Batch: 29, Avg. Loss: 0.23901809379458427\n",
      "Epoch: 3, Batch: 33, Avg. Loss: 0.08442490734159946\n",
      "Epoch: 3, Batch: 37, Avg. Loss: 0.2054564505815506\n",
      "Epoch: 3, Batch: 41, Avg. Loss: 0.1398621965199709\n",
      "Epoch: 3, Batch: 45, Avg. Loss: 0.1315826866775751\n",
      "Epoch: 3, Batch: 49, Avg. Loss: 0.09107003547251225\n",
      "Epoch: 3, Batch: 53, Avg. Loss: 0.1505706887692213\n",
      "Epoch: 3, Batch: 57, Avg. Loss: 0.1120194373652339\n",
      "Epoch: 3, Batch: 61, Avg. Loss: 0.1729520447552204\n",
      "Epoch: 3, Batch: 65, Avg. Loss: 0.13716659415513277\n",
      "Epoch: 3, Batch: 69, Avg. Loss: 0.12114830315113068\n",
      "Epoch: 3, Batch: 73, Avg. Loss: 0.12359361443668604\n",
      "Epoch: 3, Batch: 77, Avg. Loss: 0.15204590745270252\n",
      "Epoch: 3, Batch: 81, Avg. Loss: 0.16472457814961672\n",
      "Epoch: 3, Batch: 85, Avg. Loss: 0.10746636148542166\n",
      "Epoch: 3, Batch: 89, Avg. Loss: 0.1303126197308302\n",
      "Epoch: 3, Batch: 93, Avg. Loss: 0.1525691244751215\n",
      "Epoch: 3, Batch: 97, Avg. Loss: 0.12053779140114784\n",
      "Epoch: 3, Batch: 101, Avg. Loss: 0.1566387377679348\n",
      "Epoch: 3, Batch: 105, Avg. Loss: 0.17354870587587357\n",
      "Epoch: 3, Batch: 109, Avg. Loss: 0.12967997323721647\n",
      "Epoch: 3, Batch: 113, Avg. Loss: 0.1197191122919321\n",
      "Epoch: 3, Batch: 117, Avg. Loss: 0.15208046324551105\n",
      "Epoch: 3, Batch: 121, Avg. Loss: 0.19739649072289467\n",
      "Epoch: 3, Batch: 125, Avg. Loss: 0.14253289997577667\n",
      "Epoch: 3, Batch: 129, Avg. Loss: 0.17398744076490402\n",
      "Epoch: 3, Batch: 133, Avg. Loss: 0.10317341983318329\n",
      "Epoch: 3, Batch: 137, Avg. Loss: 0.11402126774191856\n",
      "Epoch: 3, Batch: 141, Avg. Loss: 0.12181912641972303\n",
      "Epoch: 3, Batch: 145, Avg. Loss: 0.09332533832639456\n",
      "Epoch: 3, Batch: 149, Avg. Loss: 0.15815553069114685\n",
      "Epoch: 3, Batch: 153, Avg. Loss: 0.2130078710615635\n",
      "Epoch: 3, Batch: 157, Avg. Loss: 0.14075091388076544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 161, Avg. Loss: 0.12465518899261951\n",
      "Epoch: 3, Batch: 165, Avg. Loss: 0.1539424043148756\n",
      "Epoch: 3, Batch: 169, Avg. Loss: 0.14936521649360657\n",
      "Epoch: 3, Batch: 173, Avg. Loss: 0.14198838733136654\n",
      "Epoch: 3, Batch: 177, Avg. Loss: 0.11953499168157578\n",
      "Epoch: 3, Batch: 181, Avg. Loss: 0.13990438263863325\n",
      "Epoch: 3, Batch: 185, Avg. Loss: 0.17089439928531647\n",
      "Epoch: 3, Batch: 189, Avg. Loss: 0.1514559481292963\n",
      "Epoch: 3, Batch: 193, Avg. Loss: 0.08988838829100132\n",
      "Epoch: 3, Batch: 197, Avg. Loss: 0.15405452251434326\n",
      "Epoch: 3, Batch: 201, Avg. Loss: 0.1761625725775957\n",
      "Epoch: 3, Batch: 205, Avg. Loss: 0.1366143822669983\n",
      "Epoch: 3, Batch: 209, Avg. Loss: 0.08381297718733549\n",
      "Epoch: 3, Batch: 213, Avg. Loss: 0.10416727513074875\n",
      "Epoch: 3, Batch: 217, Avg. Loss: 0.1317143589258194\n",
      "Epoch: 3, Batch: 221, Avg. Loss: 0.12503411434590816\n",
      "Epoch: 3, Batch: 225, Avg. Loss: 0.13588252663612366\n",
      "Epoch: 3, Batch: 229, Avg. Loss: 0.18014116678386927\n",
      "Epoch: 3, Batch: 233, Avg. Loss: 0.10635692626237869\n",
      "Epoch: 4, Batch: 1, Avg. Loss: 0.016008976846933365\n",
      "Epoch: 4, Batch: 5, Avg. Loss: 0.16686509922146797\n",
      "Epoch: 4, Batch: 9, Avg. Loss: 0.10378059279173613\n",
      "Epoch: 4, Batch: 13, Avg. Loss: 0.06958138756453991\n",
      "Epoch: 4, Batch: 17, Avg. Loss: 0.20484416373074055\n",
      "Epoch: 4, Batch: 21, Avg. Loss: 0.13649610243737698\n",
      "Epoch: 4, Batch: 25, Avg. Loss: 0.13834652118384838\n",
      "Epoch: 4, Batch: 29, Avg. Loss: 0.2580881118774414\n",
      "Epoch: 4, Batch: 33, Avg. Loss: 0.14956842362880707\n",
      "Epoch: 4, Batch: 37, Avg. Loss: 0.11188653763383627\n",
      "Epoch: 4, Batch: 41, Avg. Loss: 0.12185250408947468\n",
      "Epoch: 4, Batch: 45, Avg. Loss: 0.13236282858997583\n",
      "Epoch: 4, Batch: 49, Avg. Loss: 0.13310854509472847\n",
      "Epoch: 4, Batch: 53, Avg. Loss: 0.09230840858072042\n",
      "Epoch: 4, Batch: 57, Avg. Loss: 0.14834794774651527\n",
      "Epoch: 4, Batch: 61, Avg. Loss: 0.10714979656040668\n",
      "Epoch: 4, Batch: 65, Avg. Loss: 0.154338413849473\n",
      "Epoch: 4, Batch: 69, Avg. Loss: 0.12582662142813206\n",
      "Epoch: 4, Batch: 73, Avg. Loss: 0.09013302624225616\n",
      "Epoch: 4, Batch: 77, Avg. Loss: 0.1630208119750023\n",
      "Epoch: 4, Batch: 81, Avg. Loss: 0.0693422481417656\n",
      "Epoch: 4, Batch: 85, Avg. Loss: 0.1956177782267332\n",
      "Epoch: 4, Batch: 89, Avg. Loss: 0.1434443248435855\n",
      "Epoch: 4, Batch: 93, Avg. Loss: 0.11098918039351702\n",
      "Epoch: 4, Batch: 97, Avg. Loss: 0.11919434368610382\n",
      "Epoch: 4, Batch: 101, Avg. Loss: 0.19256973545998335\n",
      "Epoch: 4, Batch: 105, Avg. Loss: 0.1520477905869484\n",
      "Epoch: 4, Batch: 109, Avg. Loss: 0.12198732979595661\n",
      "Epoch: 4, Batch: 113, Avg. Loss: 0.11900334618985653\n",
      "Epoch: 4, Batch: 117, Avg. Loss: 0.12700662948191166\n",
      "Epoch: 4, Batch: 121, Avg. Loss: 0.1240495853126049\n",
      "Epoch: 4, Batch: 125, Avg. Loss: 0.16275146510452032\n",
      "Epoch: 4, Batch: 129, Avg. Loss: 0.0940553592517972\n",
      "Epoch: 4, Batch: 133, Avg. Loss: 0.16307208873331547\n",
      "Epoch: 4, Batch: 137, Avg. Loss: 0.11745798215270042\n",
      "Epoch: 4, Batch: 141, Avg. Loss: 0.09713287092745304\n",
      "Epoch: 4, Batch: 145, Avg. Loss: 0.11695698462426662\n",
      "Epoch: 4, Batch: 149, Avg. Loss: 0.17794955521821976\n",
      "Epoch: 4, Batch: 153, Avg. Loss: 0.21534106321632862\n",
      "Epoch: 4, Batch: 157, Avg. Loss: 0.12030873261392117\n",
      "Epoch: 4, Batch: 161, Avg. Loss: 0.17937571369111538\n",
      "Epoch: 4, Batch: 165, Avg. Loss: 0.11865108460187912\n",
      "Epoch: 4, Batch: 169, Avg. Loss: 0.10983735881745815\n",
      "Epoch: 4, Batch: 173, Avg. Loss: 0.1296021919697523\n",
      "Epoch: 4, Batch: 177, Avg. Loss: 0.21945882588624954\n",
      "Epoch: 4, Batch: 181, Avg. Loss: 0.1481334026902914\n",
      "Epoch: 4, Batch: 185, Avg. Loss: 0.17431787587702274\n",
      "Epoch: 4, Batch: 189, Avg. Loss: 0.10708362050354481\n",
      "Epoch: 4, Batch: 193, Avg. Loss: 0.14327233005315065\n",
      "Epoch: 4, Batch: 197, Avg. Loss: 0.1746487095952034\n",
      "Epoch: 4, Batch: 201, Avg. Loss: 0.15311976335942745\n",
      "Epoch: 4, Batch: 205, Avg. Loss: 0.12003295682370663\n",
      "Epoch: 4, Batch: 209, Avg. Loss: 0.17245470918715\n",
      "Epoch: 4, Batch: 213, Avg. Loss: 0.11890571936964989\n",
      "Epoch: 4, Batch: 217, Avg. Loss: 0.14761568792164326\n",
      "Epoch: 4, Batch: 221, Avg. Loss: 0.17062551528215408\n",
      "Epoch: 4, Batch: 225, Avg. Loss: 0.11480248160660267\n",
      "Epoch: 4, Batch: 229, Avg. Loss: 0.17670458741486073\n",
      "Epoch: 4, Batch: 233, Avg. Loss: 0.15730945393443108\n",
      "Epoch: 5, Batch: 1, Avg. Loss: 0.061871450394392014\n",
      "Epoch: 5, Batch: 5, Avg. Loss: 0.09645376540720463\n",
      "Epoch: 5, Batch: 9, Avg. Loss: 0.22045717015862465\n",
      "Epoch: 5, Batch: 13, Avg. Loss: 0.12085002940148115\n",
      "Epoch: 5, Batch: 17, Avg. Loss: 0.09266389813274145\n",
      "Epoch: 5, Batch: 21, Avg. Loss: 0.16928378026932478\n",
      "Epoch: 5, Batch: 25, Avg. Loss: 0.13857016526162624\n",
      "Epoch: 5, Batch: 29, Avg. Loss: 0.15860352851450443\n",
      "Epoch: 5, Batch: 33, Avg. Loss: 0.09082557819783688\n",
      "Epoch: 5, Batch: 37, Avg. Loss: 0.14951931592077017\n",
      "Epoch: 5, Batch: 41, Avg. Loss: 0.12853187881410122\n",
      "Epoch: 5, Batch: 45, Avg. Loss: 0.13690147828310728\n",
      "Epoch: 5, Batch: 49, Avg. Loss: 0.11126365885138512\n",
      "Epoch: 5, Batch: 53, Avg. Loss: 0.13031871989369392\n",
      "Epoch: 5, Batch: 57, Avg. Loss: 0.12656691763550043\n",
      "Epoch: 5, Batch: 61, Avg. Loss: 0.11369228176772594\n",
      "Epoch: 5, Batch: 65, Avg. Loss: 0.1413185652345419\n",
      "Epoch: 5, Batch: 69, Avg. Loss: 0.11036260984838009\n",
      "Epoch: 5, Batch: 73, Avg. Loss: 0.140660903416574\n",
      "Epoch: 5, Batch: 77, Avg. Loss: 0.06130044721066952\n",
      "Epoch: 5, Batch: 81, Avg. Loss: 0.13412702456116676\n",
      "Epoch: 5, Batch: 85, Avg. Loss: 0.19497763738036156\n",
      "Epoch: 5, Batch: 89, Avg. Loss: 0.101600281894207\n",
      "Epoch: 5, Batch: 93, Avg. Loss: 0.15949498862028122\n",
      "Epoch: 5, Batch: 97, Avg. Loss: 0.21736383065581322\n",
      "Epoch: 5, Batch: 101, Avg. Loss: 0.20482588931918144\n",
      "Epoch: 5, Batch: 105, Avg. Loss: 0.10305607132613659\n",
      "Epoch: 5, Batch: 109, Avg. Loss: 0.09389831870794296\n",
      "Epoch: 5, Batch: 113, Avg. Loss: 0.13503461796790361\n",
      "Epoch: 5, Batch: 117, Avg. Loss: 0.12127816118299961\n",
      "Epoch: 5, Batch: 121, Avg. Loss: 0.15858318284153938\n",
      "Epoch: 5, Batch: 125, Avg. Loss: 0.13974102400243282\n",
      "Epoch: 5, Batch: 129, Avg. Loss: 0.1917772125452757\n",
      "Epoch: 5, Batch: 133, Avg. Loss: 0.13179747574031353\n",
      "Epoch: 5, Batch: 137, Avg. Loss: 0.13642452843487263\n",
      "Epoch: 5, Batch: 141, Avg. Loss: 0.15570420864969492\n",
      "Epoch: 5, Batch: 145, Avg. Loss: 0.11781947501003742\n",
      "Epoch: 5, Batch: 149, Avg. Loss: 0.1680037509649992\n",
      "Epoch: 5, Batch: 153, Avg. Loss: 0.1607069531455636\n",
      "Epoch: 5, Batch: 157, Avg. Loss: 0.13454779982566833\n",
      "Epoch: 5, Batch: 161, Avg. Loss: 0.11107856966555119\n",
      "Epoch: 5, Batch: 165, Avg. Loss: 0.12666493095457554\n",
      "Epoch: 5, Batch: 169, Avg. Loss: 0.07142140995711088\n",
      "Epoch: 5, Batch: 173, Avg. Loss: 0.131201496347785\n",
      "Epoch: 5, Batch: 177, Avg. Loss: 0.13791287317872047\n",
      "Epoch: 5, Batch: 181, Avg. Loss: 0.12671021092683077\n",
      "Epoch: 5, Batch: 185, Avg. Loss: 0.1834011822938919\n",
      "Epoch: 5, Batch: 189, Avg. Loss: 0.180655799806118\n",
      "Epoch: 5, Batch: 193, Avg. Loss: 0.18425932340323925\n",
      "Epoch: 5, Batch: 197, Avg. Loss: 0.18947566486895084\n",
      "Epoch: 5, Batch: 201, Avg. Loss: 0.16658997163176537\n",
      "Epoch: 5, Batch: 205, Avg. Loss: 0.1524763237684965\n",
      "Epoch: 5, Batch: 209, Avg. Loss: 0.14599876571446657\n",
      "Epoch: 5, Batch: 213, Avg. Loss: 0.16437054984271526\n",
      "Epoch: 5, Batch: 217, Avg. Loss: 0.09808364789932966\n",
      "Epoch: 5, Batch: 221, Avg. Loss: 0.11693366896361113\n",
      "Epoch: 5, Batch: 225, Avg. Loss: 0.1265843277797103\n",
      "Epoch: 5, Batch: 229, Avg. Loss: 0.149421576410532\n",
      "Epoch: 5, Batch: 233, Avg. Loss: 0.1360247302800417\n",
      "Epoch: 6, Batch: 1, Avg. Loss: 0.03683190420269966\n",
      "Epoch: 6, Batch: 5, Avg. Loss: 0.14462894294410944\n",
      "Epoch: 6, Batch: 9, Avg. Loss: 0.10807423666119576\n",
      "Epoch: 6, Batch: 13, Avg. Loss: 0.12880692817270756\n",
      "Epoch: 6, Batch: 17, Avg. Loss: 0.1549394354224205\n",
      "Epoch: 6, Batch: 21, Avg. Loss: 0.12147459480911493\n",
      "Epoch: 6, Batch: 25, Avg. Loss: 0.1531366240233183\n",
      "Epoch: 6, Batch: 29, Avg. Loss: 0.15529807284474373\n",
      "Epoch: 6, Batch: 33, Avg. Loss: 0.10905860736966133\n",
      "Epoch: 6, Batch: 37, Avg. Loss: 0.2257765606045723\n",
      "Epoch: 6, Batch: 41, Avg. Loss: 0.1451268419623375\n",
      "Epoch: 6, Batch: 45, Avg. Loss: 0.11408594343811274\n",
      "Epoch: 6, Batch: 49, Avg. Loss: 0.13628298602998257\n",
      "Epoch: 6, Batch: 53, Avg. Loss: 0.1601390028372407\n",
      "Epoch: 6, Batch: 57, Avg. Loss: 0.11259668320417404\n",
      "Epoch: 6, Batch: 61, Avg. Loss: 0.09973551332950592\n",
      "Epoch: 6, Batch: 65, Avg. Loss: 0.11527256201952696\n",
      "Epoch: 6, Batch: 69, Avg. Loss: 0.22192130982875824\n",
      "Epoch: 6, Batch: 73, Avg. Loss: 0.14685833267867565\n",
      "Epoch: 6, Batch: 77, Avg. Loss: 0.17004515044391155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Batch: 81, Avg. Loss: 0.12756245955824852\n",
      "Epoch: 6, Batch: 85, Avg. Loss: 0.11808130796998739\n",
      "Epoch: 6, Batch: 89, Avg. Loss: 0.21417968906462193\n",
      "Epoch: 6, Batch: 93, Avg. Loss: 0.12916136719286442\n",
      "Epoch: 6, Batch: 97, Avg. Loss: 0.15432480163872242\n",
      "Epoch: 6, Batch: 101, Avg. Loss: 0.17947369813919067\n",
      "Epoch: 6, Batch: 105, Avg. Loss: 0.13576513901352882\n",
      "Epoch: 6, Batch: 109, Avg. Loss: 0.15351576544344425\n",
      "Epoch: 6, Batch: 113, Avg. Loss: 0.1914057396352291\n",
      "Epoch: 6, Batch: 117, Avg. Loss: 0.1272663353011012\n",
      "Epoch: 6, Batch: 121, Avg. Loss: 0.17335908114910126\n",
      "Epoch: 6, Batch: 125, Avg. Loss: 0.11259560380131006\n",
      "Epoch: 6, Batch: 129, Avg. Loss: 0.12319597229361534\n",
      "Epoch: 6, Batch: 133, Avg. Loss: 0.11287750117480755\n",
      "Epoch: 6, Batch: 137, Avg. Loss: 0.12494784407317638\n",
      "Epoch: 6, Batch: 141, Avg. Loss: 0.14460100140422583\n",
      "Epoch: 6, Batch: 145, Avg. Loss: 0.08455625828355551\n",
      "Epoch: 6, Batch: 149, Avg. Loss: 0.1682528480887413\n",
      "Epoch: 6, Batch: 153, Avg. Loss: 0.16013169940561056\n",
      "Epoch: 6, Batch: 157, Avg. Loss: 0.18327906168997288\n",
      "Epoch: 6, Batch: 161, Avg. Loss: 0.191763985902071\n",
      "Epoch: 6, Batch: 165, Avg. Loss: 0.09874489717185497\n",
      "Epoch: 6, Batch: 169, Avg. Loss: 0.13603620324283838\n",
      "Epoch: 6, Batch: 173, Avg. Loss: 0.17868187092244625\n",
      "Epoch: 6, Batch: 177, Avg. Loss: 0.07840560376644135\n",
      "Epoch: 6, Batch: 181, Avg. Loss: 0.1659977026283741\n",
      "Epoch: 6, Batch: 185, Avg. Loss: 0.1276919236406684\n",
      "Epoch: 6, Batch: 189, Avg. Loss: 0.09468365926295519\n",
      "Epoch: 6, Batch: 193, Avg. Loss: 0.14466738514602184\n",
      "Epoch: 6, Batch: 197, Avg. Loss: 0.12535474263131618\n",
      "Epoch: 6, Batch: 201, Avg. Loss: 0.1415874231606722\n",
      "Epoch: 6, Batch: 205, Avg. Loss: 0.09018714725971222\n",
      "Epoch: 6, Batch: 209, Avg. Loss: 0.09813157841563225\n",
      "Epoch: 6, Batch: 213, Avg. Loss: 0.1432567983865738\n",
      "Epoch: 6, Batch: 217, Avg. Loss: 0.10254915617406368\n",
      "Epoch: 6, Batch: 221, Avg. Loss: 0.13404327258467674\n",
      "Epoch: 6, Batch: 225, Avg. Loss: 0.16248615644872189\n",
      "Epoch: 6, Batch: 229, Avg. Loss: 0.12970054894685745\n",
      "Epoch: 6, Batch: 233, Avg. Loss: 0.09223932027816772\n",
      "Epoch: 7, Batch: 1, Avg. Loss: 0.0500984787940979\n",
      "Epoch: 7, Batch: 5, Avg. Loss: 0.13216431625187397\n",
      "Epoch: 7, Batch: 9, Avg. Loss: 0.1522524729371071\n",
      "Epoch: 7, Batch: 13, Avg. Loss: 0.10869238525629044\n",
      "Epoch: 7, Batch: 17, Avg. Loss: 0.13310537673532963\n",
      "Epoch: 7, Batch: 21, Avg. Loss: 0.10493444558233023\n",
      "Epoch: 7, Batch: 25, Avg. Loss: 0.05948909558355808\n",
      "Epoch: 7, Batch: 29, Avg. Loss: 0.08366327825933695\n",
      "Epoch: 7, Batch: 33, Avg. Loss: 0.10234807338565588\n",
      "Epoch: 7, Batch: 37, Avg. Loss: 0.1193622313439846\n",
      "Epoch: 7, Batch: 41, Avg. Loss: 0.18475468456745148\n",
      "Epoch: 7, Batch: 45, Avg. Loss: 0.0957147367298603\n",
      "Epoch: 7, Batch: 49, Avg. Loss: 0.12181336060166359\n",
      "Epoch: 7, Batch: 53, Avg. Loss: 0.10885378904640675\n",
      "Epoch: 7, Batch: 57, Avg. Loss: 0.14908587001264095\n",
      "Epoch: 7, Batch: 61, Avg. Loss: 0.1621905043721199\n",
      "Epoch: 7, Batch: 65, Avg. Loss: 0.07553574163466692\n",
      "Epoch: 7, Batch: 69, Avg. Loss: 0.23243499547243118\n",
      "Epoch: 7, Batch: 73, Avg. Loss: 0.10652751475572586\n",
      "Epoch: 7, Batch: 77, Avg. Loss: 0.15188984386622906\n",
      "Epoch: 7, Batch: 81, Avg. Loss: 0.09640027768909931\n",
      "Epoch: 7, Batch: 85, Avg. Loss: 0.20309508964419365\n",
      "Epoch: 7, Batch: 89, Avg. Loss: 0.17741944082081318\n",
      "Epoch: 7, Batch: 93, Avg. Loss: 0.11512529104948044\n",
      "Epoch: 7, Batch: 97, Avg. Loss: 0.21224337257444859\n",
      "Epoch: 7, Batch: 101, Avg. Loss: 0.1261358130723238\n",
      "Epoch: 7, Batch: 105, Avg. Loss: 0.09673896431922913\n",
      "Epoch: 7, Batch: 109, Avg. Loss: 0.2154257521033287\n",
      "Epoch: 7, Batch: 113, Avg. Loss: 0.12881519366055727\n",
      "Epoch: 7, Batch: 117, Avg. Loss: 0.13820037990808487\n",
      "Epoch: 7, Batch: 121, Avg. Loss: 0.12295135296881199\n",
      "Epoch: 7, Batch: 125, Avg. Loss: 0.15031909942626953\n",
      "Epoch: 7, Batch: 129, Avg. Loss: 0.13269564509391785\n",
      "Epoch: 7, Batch: 133, Avg. Loss: 0.13509549014270306\n",
      "Epoch: 7, Batch: 137, Avg. Loss: 0.18614336661994457\n",
      "Epoch: 7, Batch: 141, Avg. Loss: 0.1258437018841505\n",
      "Epoch: 7, Batch: 145, Avg. Loss: 0.11798408813774586\n",
      "Epoch: 7, Batch: 149, Avg. Loss: 0.18600911553949118\n",
      "Epoch: 7, Batch: 153, Avg. Loss: 0.1266136486083269\n",
      "Epoch: 7, Batch: 157, Avg. Loss: 0.18891676142811775\n",
      "Epoch: 7, Batch: 161, Avg. Loss: 0.18796773813664913\n",
      "Epoch: 7, Batch: 165, Avg. Loss: 0.1664537489414215\n",
      "Epoch: 7, Batch: 169, Avg. Loss: 0.08770995493978262\n",
      "Epoch: 7, Batch: 173, Avg. Loss: 0.13205215707421303\n",
      "Epoch: 7, Batch: 177, Avg. Loss: 0.10635196324437857\n",
      "Epoch: 7, Batch: 181, Avg. Loss: 0.21645071357488632\n",
      "Epoch: 7, Batch: 185, Avg. Loss: 0.11932074930518866\n",
      "Epoch: 7, Batch: 189, Avg. Loss: 0.13146016746759415\n",
      "Epoch: 7, Batch: 193, Avg. Loss: 0.14553364925086498\n",
      "Epoch: 7, Batch: 197, Avg. Loss: 0.07634393312036991\n",
      "Epoch: 7, Batch: 201, Avg. Loss: 0.15175585635006428\n",
      "Epoch: 7, Batch: 205, Avg. Loss: 0.14996721968054771\n",
      "Epoch: 7, Batch: 209, Avg. Loss: 0.12185283564031124\n",
      "Epoch: 7, Batch: 213, Avg. Loss: 0.14853337034583092\n",
      "Epoch: 7, Batch: 217, Avg. Loss: 0.15182184148579836\n",
      "Epoch: 7, Batch: 221, Avg. Loss: 0.14180218800902367\n",
      "Epoch: 7, Batch: 225, Avg. Loss: 0.12923439219594002\n",
      "Epoch: 7, Batch: 229, Avg. Loss: 0.15902851335704327\n",
      "Epoch: 7, Batch: 233, Avg. Loss: 0.10872011259198189\n",
      "Epoch: 8, Batch: 1, Avg. Loss: 0.02003852277994156\n",
      "Epoch: 8, Batch: 5, Avg. Loss: 0.09616886544972658\n",
      "Epoch: 8, Batch: 9, Avg. Loss: 0.1657087616622448\n",
      "Epoch: 8, Batch: 13, Avg. Loss: 0.13322852179408073\n",
      "Epoch: 8, Batch: 17, Avg. Loss: 0.1376260807737708\n",
      "Epoch: 8, Batch: 21, Avg. Loss: 0.11964239552617073\n",
      "Epoch: 8, Batch: 25, Avg. Loss: 0.15290590561926365\n",
      "Epoch: 8, Batch: 29, Avg. Loss: 0.1267624506726861\n",
      "Epoch: 8, Batch: 33, Avg. Loss: 0.1642984990030527\n",
      "Epoch: 8, Batch: 37, Avg. Loss: 0.09251588676124811\n",
      "Epoch: 8, Batch: 41, Avg. Loss: 0.13745395839214325\n",
      "Epoch: 8, Batch: 45, Avg. Loss: 0.1139796283096075\n",
      "Epoch: 8, Batch: 49, Avg. Loss: 0.06961813848465681\n",
      "Epoch: 8, Batch: 53, Avg. Loss: 0.1263004019856453\n",
      "Epoch: 8, Batch: 57, Avg. Loss: 0.2227422147989273\n",
      "Epoch: 8, Batch: 61, Avg. Loss: 0.09641732554882765\n",
      "Epoch: 8, Batch: 65, Avg. Loss: 0.1355542242527008\n",
      "Epoch: 8, Batch: 69, Avg. Loss: 0.11627217847853899\n",
      "Epoch: 8, Batch: 73, Avg. Loss: 0.13267423026263714\n",
      "Epoch: 8, Batch: 77, Avg. Loss: 0.13826254103332758\n",
      "Epoch: 8, Batch: 81, Avg. Loss: 0.1516238320618868\n",
      "Epoch: 8, Batch: 85, Avg. Loss: 0.13497518375515938\n",
      "Epoch: 8, Batch: 89, Avg. Loss: 0.11544106248766184\n",
      "Epoch: 8, Batch: 93, Avg. Loss: 0.19547163508832455\n",
      "Epoch: 8, Batch: 97, Avg. Loss: 0.1801864393055439\n",
      "Epoch: 8, Batch: 101, Avg. Loss: 0.16590386256575584\n",
      "Epoch: 8, Batch: 105, Avg. Loss: 0.12013140879571438\n",
      "Epoch: 8, Batch: 109, Avg. Loss: 0.09010626096278429\n",
      "Epoch: 8, Batch: 113, Avg. Loss: 0.15208494290709496\n",
      "Epoch: 8, Batch: 117, Avg. Loss: 0.14526250027120113\n",
      "Epoch: 8, Batch: 121, Avg. Loss: 0.13152822945266962\n",
      "Epoch: 8, Batch: 125, Avg. Loss: 0.1703588142991066\n",
      "Epoch: 8, Batch: 129, Avg. Loss: 0.11104414984583855\n",
      "Epoch: 8, Batch: 133, Avg. Loss: 0.128781845793128\n",
      "Epoch: 8, Batch: 137, Avg. Loss: 0.1096461471170187\n",
      "Epoch: 8, Batch: 141, Avg. Loss: 0.16644291952252388\n",
      "Epoch: 8, Batch: 145, Avg. Loss: 0.10689720790833235\n",
      "Epoch: 8, Batch: 149, Avg. Loss: 0.13064427487552166\n",
      "Epoch: 8, Batch: 153, Avg. Loss: 0.14684353955090046\n",
      "Epoch: 8, Batch: 157, Avg. Loss: 0.15758906677365303\n",
      "Epoch: 8, Batch: 161, Avg. Loss: 0.18220882676541805\n",
      "Epoch: 8, Batch: 165, Avg. Loss: 0.2374483421444893\n",
      "Epoch: 8, Batch: 169, Avg. Loss: 0.10111341439187527\n",
      "Epoch: 8, Batch: 173, Avg. Loss: 0.14762224722653627\n",
      "Epoch: 8, Batch: 177, Avg. Loss: 0.12813289929181337\n",
      "Epoch: 8, Batch: 181, Avg. Loss: 0.1420498313382268\n",
      "Epoch: 8, Batch: 185, Avg. Loss: 0.14199748449027538\n",
      "Epoch: 8, Batch: 189, Avg. Loss: 0.17814068496227264\n",
      "Epoch: 8, Batch: 193, Avg. Loss: 0.11098826862871647\n",
      "Epoch: 8, Batch: 197, Avg. Loss: 0.1690447200089693\n",
      "Epoch: 8, Batch: 201, Avg. Loss: 0.09864740818738937\n",
      "Epoch: 8, Batch: 205, Avg. Loss: 0.10830391012132168\n",
      "Epoch: 8, Batch: 209, Avg. Loss: 0.14115986041724682\n",
      "Epoch: 8, Batch: 213, Avg. Loss: 0.1070774495601654\n",
      "Epoch: 8, Batch: 217, Avg. Loss: 0.1290877116844058\n",
      "Epoch: 8, Batch: 221, Avg. Loss: 0.11587335914373398\n",
      "Epoch: 8, Batch: 225, Avg. Loss: 0.20838091894984245\n",
      "Epoch: 8, Batch: 229, Avg. Loss: 0.1092153387144208\n",
      "Epoch: 8, Batch: 233, Avg. Loss: 0.12770563550293446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Batch: 1, Avg. Loss: 0.05424889549612999\n",
      "Epoch: 9, Batch: 5, Avg. Loss: 0.16231120936572552\n",
      "Epoch: 9, Batch: 9, Avg. Loss: 0.08066965732723475\n",
      "Epoch: 9, Batch: 13, Avg. Loss: 0.12423248216509819\n",
      "Epoch: 9, Batch: 17, Avg. Loss: 0.11194130592048168\n",
      "Epoch: 9, Batch: 21, Avg. Loss: 0.15133318677544594\n",
      "Epoch: 9, Batch: 25, Avg. Loss: 0.08188071474432945\n",
      "Epoch: 9, Batch: 29, Avg. Loss: 0.1576324850320816\n",
      "Epoch: 9, Batch: 33, Avg. Loss: 0.17346794344484806\n",
      "Epoch: 9, Batch: 37, Avg. Loss: 0.14423097297549248\n",
      "Epoch: 9, Batch: 41, Avg. Loss: 0.14797473698854446\n",
      "Epoch: 9, Batch: 45, Avg. Loss: 0.09497140347957611\n",
      "Epoch: 9, Batch: 49, Avg. Loss: 0.20268354378640652\n",
      "Epoch: 9, Batch: 53, Avg. Loss: 0.09841650724411011\n",
      "Epoch: 9, Batch: 57, Avg. Loss: 0.09524000808596611\n",
      "Epoch: 9, Batch: 61, Avg. Loss: 0.1941597107797861\n",
      "Epoch: 9, Batch: 65, Avg. Loss: 0.15863467194139957\n",
      "Epoch: 9, Batch: 69, Avg. Loss: 0.11357637029141188\n",
      "Epoch: 9, Batch: 73, Avg. Loss: 0.11974216625094414\n",
      "Epoch: 9, Batch: 77, Avg. Loss: 0.12468111142516136\n",
      "Epoch: 9, Batch: 81, Avg. Loss: 0.21699316427111626\n",
      "Epoch: 9, Batch: 85, Avg. Loss: 0.059962271712720394\n",
      "Epoch: 9, Batch: 89, Avg. Loss: 0.12838194984942675\n",
      "Epoch: 9, Batch: 93, Avg. Loss: 0.11249736323952675\n",
      "Epoch: 9, Batch: 97, Avg. Loss: 0.10413822811096907\n",
      "Epoch: 9, Batch: 101, Avg. Loss: 0.13130451925098896\n",
      "Epoch: 9, Batch: 105, Avg. Loss: 0.1514285858720541\n",
      "Epoch: 9, Batch: 109, Avg. Loss: 0.14259326085448265\n",
      "Epoch: 9, Batch: 113, Avg. Loss: 0.20572985336184502\n",
      "Epoch: 9, Batch: 117, Avg. Loss: 0.1476430157199502\n",
      "Epoch: 9, Batch: 121, Avg. Loss: 0.182165689766407\n",
      "Epoch: 9, Batch: 125, Avg. Loss: 0.1922870296984911\n",
      "Epoch: 9, Batch: 129, Avg. Loss: 0.18065391667187214\n",
      "Epoch: 9, Batch: 133, Avg. Loss: 0.11006583366543055\n",
      "Epoch: 9, Batch: 137, Avg. Loss: 0.13168245553970337\n",
      "Epoch: 9, Batch: 141, Avg. Loss: 0.11033552512526512\n",
      "Epoch: 9, Batch: 145, Avg. Loss: 0.16234569065272808\n",
      "Epoch: 9, Batch: 149, Avg. Loss: 0.15369980596005917\n",
      "Epoch: 9, Batch: 153, Avg. Loss: 0.1428746860474348\n",
      "Epoch: 9, Batch: 157, Avg. Loss: 0.15965446271002293\n",
      "Epoch: 9, Batch: 161, Avg. Loss: 0.07646317686885595\n",
      "Epoch: 9, Batch: 165, Avg. Loss: 0.09436466079205275\n",
      "Epoch: 9, Batch: 169, Avg. Loss: 0.1356164701282978\n",
      "Epoch: 9, Batch: 173, Avg. Loss: 0.24658504128456116\n",
      "Epoch: 9, Batch: 177, Avg. Loss: 0.1656327247619629\n",
      "Epoch: 9, Batch: 181, Avg. Loss: 0.12618357688188553\n",
      "Epoch: 9, Batch: 185, Avg. Loss: 0.12248988822102547\n",
      "Epoch: 9, Batch: 189, Avg. Loss: 0.09752921015024185\n",
      "Epoch: 9, Batch: 193, Avg. Loss: 0.1241876007989049\n",
      "Epoch: 9, Batch: 197, Avg. Loss: 0.13040528167039156\n",
      "Epoch: 9, Batch: 201, Avg. Loss: 0.09750543721020222\n",
      "Epoch: 9, Batch: 205, Avg. Loss: 0.1552367638796568\n",
      "Epoch: 9, Batch: 209, Avg. Loss: 0.20049463212490082\n",
      "Epoch: 9, Batch: 213, Avg. Loss: 0.16708122938871384\n",
      "Epoch: 9, Batch: 217, Avg. Loss: 0.12576670665293932\n",
      "Epoch: 9, Batch: 221, Avg. Loss: 0.08745635114610195\n",
      "Epoch: 9, Batch: 225, Avg. Loss: 0.18173462711274624\n",
      "Epoch: 9, Batch: 229, Avg. Loss: 0.10115069895982742\n",
      "Epoch: 9, Batch: 233, Avg. Loss: 0.08261142484843731\n",
      "Epoch: 10, Batch: 1, Avg. Loss: 0.04681600257754326\n",
      "Epoch: 10, Batch: 5, Avg. Loss: 0.16578441858291626\n",
      "Epoch: 10, Batch: 9, Avg. Loss: 0.15722198691219091\n",
      "Epoch: 10, Batch: 13, Avg. Loss: 0.17831814102828503\n",
      "Epoch: 10, Batch: 17, Avg. Loss: 0.12727885507047176\n",
      "Epoch: 10, Batch: 21, Avg. Loss: 0.08673457708209753\n",
      "Epoch: 10, Batch: 25, Avg. Loss: 0.1295751966536045\n",
      "Epoch: 10, Batch: 29, Avg. Loss: 0.11876604706048965\n",
      "Epoch: 10, Batch: 33, Avg. Loss: 0.10965830832719803\n",
      "Epoch: 10, Batch: 37, Avg. Loss: 0.13075048942118883\n",
      "Epoch: 10, Batch: 41, Avg. Loss: 0.12530416622757912\n",
      "Epoch: 10, Batch: 45, Avg. Loss: 0.1311618322506547\n",
      "Epoch: 10, Batch: 49, Avg. Loss: 0.12771919183433056\n",
      "Epoch: 10, Batch: 53, Avg. Loss: 0.12151374574750662\n",
      "Epoch: 10, Batch: 57, Avg. Loss: 0.06667125876992941\n",
      "Epoch: 10, Batch: 61, Avg. Loss: 0.11907082796096802\n",
      "Epoch: 10, Batch: 65, Avg. Loss: 0.15191875211894512\n",
      "Epoch: 10, Batch: 69, Avg. Loss: 0.19667399488389492\n",
      "Epoch: 10, Batch: 73, Avg. Loss: 0.12820950523018837\n",
      "Epoch: 10, Batch: 77, Avg. Loss: 0.20029177702963352\n",
      "Epoch: 10, Batch: 81, Avg. Loss: 0.10089983232319355\n",
      "Epoch: 10, Batch: 85, Avg. Loss: 0.10040577035397291\n",
      "Epoch: 10, Batch: 89, Avg. Loss: 0.0790449371561408\n",
      "Epoch: 10, Batch: 93, Avg. Loss: 0.14916051365435123\n",
      "Epoch: 10, Batch: 97, Avg. Loss: 0.12327091116458178\n",
      "Epoch: 10, Batch: 101, Avg. Loss: 0.16938685532659292\n",
      "Epoch: 10, Batch: 105, Avg. Loss: 0.20185839757323265\n",
      "Epoch: 10, Batch: 109, Avg. Loss: 0.169365169480443\n",
      "Epoch: 10, Batch: 113, Avg. Loss: 0.15917522087693214\n",
      "Epoch: 10, Batch: 117, Avg. Loss: 0.14679436944425106\n",
      "Epoch: 10, Batch: 121, Avg. Loss: 0.18820901401340961\n",
      "Epoch: 10, Batch: 125, Avg. Loss: 0.10132613591849804\n",
      "Epoch: 10, Batch: 129, Avg. Loss: 0.11634628754109144\n",
      "Epoch: 10, Batch: 133, Avg. Loss: 0.14974403753876686\n",
      "Epoch: 10, Batch: 137, Avg. Loss: 0.15347357280552387\n",
      "Epoch: 10, Batch: 141, Avg. Loss: 0.09150809422135353\n",
      "Epoch: 10, Batch: 145, Avg. Loss: 0.1171595174819231\n",
      "Epoch: 10, Batch: 149, Avg. Loss: 0.1518462672829628\n",
      "Epoch: 10, Batch: 153, Avg. Loss: 0.23063650354743004\n",
      "Epoch: 10, Batch: 157, Avg. Loss: 0.08311903197318316\n",
      "Epoch: 10, Batch: 161, Avg. Loss: 0.12747660279273987\n",
      "Epoch: 10, Batch: 165, Avg. Loss: 0.16948941349983215\n",
      "Epoch: 10, Batch: 169, Avg. Loss: 0.07469592895358801\n",
      "Epoch: 10, Batch: 173, Avg. Loss: 0.10197628568857908\n",
      "Epoch: 10, Batch: 177, Avg. Loss: 0.10270319785922766\n",
      "Epoch: 10, Batch: 181, Avg. Loss: 0.13731872849166393\n",
      "Epoch: 10, Batch: 185, Avg. Loss: 0.15015575289726257\n",
      "Epoch: 10, Batch: 189, Avg. Loss: 0.119513725861907\n",
      "Epoch: 10, Batch: 193, Avg. Loss: 0.08493381645530462\n",
      "Epoch: 10, Batch: 197, Avg. Loss: 0.16507541202008724\n",
      "Epoch: 10, Batch: 201, Avg. Loss: 0.12971511390060186\n",
      "Epoch: 10, Batch: 205, Avg. Loss: 0.12985155917704105\n",
      "Epoch: 10, Batch: 209, Avg. Loss: 0.21972840279340744\n",
      "Epoch: 10, Batch: 213, Avg. Loss: 0.1603827178478241\n",
      "Epoch: 10, Batch: 217, Avg. Loss: 0.18229569494724274\n",
      "Epoch: 10, Batch: 221, Avg. Loss: 0.15685189701616764\n",
      "Epoch: 10, Batch: 225, Avg. Loss: 0.18267827481031418\n",
      "Epoch: 10, Batch: 229, Avg. Loss: 0.122026477009058\n",
      "Epoch: 10, Batch: 233, Avg. Loss: 0.09006027597934008\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "print_every = 4\n",
    "\n",
    "loss_over_time = [] # to track the loss as the network trains\n",
    "    \n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_i, (input_data, labels) in enumerate(train_loader):\n",
    "        # Zero gradients (just in case)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass, calculate predictions\n",
    "        output = lstm_model(input_data) \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, labels)\n",
    "        ## Backward propagation\n",
    "        loss.backward()\n",
    "        ## Upade weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print loss statistics\n",
    "        # to convert loss into a scalar and add it to running_loss, we use .item()\n",
    "        running_loss += loss.item()\n",
    "            \n",
    "        \n",
    "        if batch_i % print_every == 0:    # print every 100 batches (staring from 50)\n",
    "                avg_loss = running_loss/print_every\n",
    "                # record and print the avg loss over the 100 batches\n",
    "                loss_over_time.append(avg_loss)\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, avg_loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12ca72198>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "input_data, labels = dataiter.next()\n",
    "lstm_model(input_data) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a94eb086a6cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mevaluate_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_classification(y, y_hat, y_proba):\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y, y_hat),\n",
    "        \"Precision\": precision_score(y, y_hat),\n",
    "        \"Recall\": recall_score(y, y_hat),\n",
    "        \"F1-score\": f1_score(y, y_hat),\n",
    "        \"AUC\": roc_auc_score(y, y_proba),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.000056\n",
      "\n",
      "Test Accuracy of toxic: 90% (2262/2500)\n",
      "Test Accuracy of severe_toxic: 99% (2476/2500)\n",
      "Test Accuracy of obscene: 94% (2361/2500)\n",
      "Test Accuracy of threat: 99% (2492/2500)\n",
      "Test Accuracy of insult: 94% (2368/2500)\n",
      "Test Accuracy of identity_hate: 99% (2484/2500)\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yc00086/miniconda/envs/conda3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/yc00086/miniconda/envs/conda3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/yc00086/miniconda/envs/conda3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/yc00086/miniconda/envs/conda3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/yc00086/miniconda/envs/conda3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/yc00086/miniconda/envs/conda3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-9fe6ce5db8b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclass_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             print('Test ' + metric +' of %5s: %2d%%' % (\n\u001b[0;32m---> 51\u001b[0;31m                 label_colnames[i], class_correct[i][metric]))\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Accuracy of %5s: N/A (no training examples)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_colnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_classes = 6\n",
    "# initialize tensor and lists to monitor test loss and accuracy\n",
    "test_loss = torch.zeros(1)\n",
    "class_correct = list(0. for i in range(num_classes))\n",
    "class_total = list(0. for i in range(num_classes))\n",
    "\n",
    "# set the module to evaluation mode\n",
    "lstm_model.eval()\n",
    "\n",
    "# get the input images and their corresponding labels\n",
    "inputs, labels = test_loader.dataset.tensors\n",
    "\n",
    "# forward pass to get outputs\n",
    "outputs = lstm_model(inputs)\n",
    "\n",
    "# calculate the loss\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "# update average test loss \n",
    "test_loss = test_loss + ((torch.ones(1) / (len(labels) + 1)) * (loss.data - test_loss))\n",
    "\n",
    "# get the predicted class from the maximum value in the output-list of class scores\n",
    "class_correct = {}\n",
    "for j in range(num_classes):\n",
    "    \n",
    "\n",
    "    # compare predictions to true label\n",
    "    predicted_class = np.round(outputs.data[:,j])\n",
    "    labels_class = labels.data[:,j]\n",
    "    class_total[j] = len(labels)\n",
    "    class_correct[j] = (labels_class==predicted_class).sum()\n",
    "    class_correct[label_colnames[j]] = evaluate_classification(labels_class, predicted_class, outputs.data[:,j])#(predicted_class == labels_class).sum()\n",
    "              \n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss.numpy()[0]))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    \n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            label_colnames[i], 100 * class_correct[i] / class_total[i],\n",
    "            (class_correct[i]), (class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (label_colnames[i]))\n",
    "    \n",
    "for metric in class_correct[label_colnames[j]].keys():         \n",
    "    print('------------------------------------------------------------------------------')\n",
    "    for i in range(num_classes):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test ' + metric +' of %5s: %2d%%' % (\n",
    "                label_colnames[i], class_correct[i][metric]))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (label_colnames[i]))        \n",
    "    print('------------------------------------------------------------------------------')\n",
    "        \n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor(2262),\n",
       " 'toxic': {'Accuracy': 0.9048,\n",
       "  'Precision': 1.0,\n",
       "  'Recall': 0.0205761316872428,\n",
       "  'F1-score': 0.04032258064516129,\n",
       "  'AUC': 0.5287537081708302},\n",
       " 1: tensor(2476),\n",
       " 'severe_toxic': {'Accuracy': 0.9904,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1-score': 0.0,\n",
       "  'AUC': 0.5408779074033312},\n",
       " 2: tensor(2361),\n",
       " 'obscene': {'Accuracy': 0.9444,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1-score': 0.0,\n",
       "  'AUC': 0.5188281395214197},\n",
       " 3: tensor(2492),\n",
       " 'threat': {'Accuracy': 0.9968,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1-score': 0.0,\n",
       "  'AUC': 0.5512389646869984},\n",
       " 4: tensor(2368),\n",
       " 'insult': {'Accuracy': 0.9472,\n",
       "  'Precision': 0.6666666666666666,\n",
       "  'Recall': 0.015037593984962405,\n",
       "  'F1-score': 0.029411764705882353,\n",
       "  'AUC': 0.5252310116228467},\n",
       " 5: tensor(2484),\n",
       " 'identity_hate': {'Accuracy': 0.9936,\n",
       "  'Precision': 0.0,\n",
       "  'Recall': 0.0,\n",
       "  'F1-score': 0.0,\n",
       "  'AUC': 0.5392134661835748}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
