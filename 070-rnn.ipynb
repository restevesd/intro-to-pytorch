{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "![LSTM](imgs/LSTM3-chain.png)\n",
    "\n",
    "![LSTM](imgs/LSTM2-notation.png)\n",
    "\n",
    "\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-f.png)\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-i.png)\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-C.png)\n",
    "\n",
    "![LSTM](imgs/LSTM3-focus-o.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From pytorch documentation\n",
    "\n",
    "\\begin{array}{ll} \\\\\n",
    "    f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "    i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "    g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "    c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
    "    o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\    \n",
    "    h_t = o_t * \\tanh(c_t) \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0511, -1.2613,  0.7664],\n",
       "         [ 0.2448, -0.8163, -0.0296]],\n",
       "\n",
       "        [[ 1.1750, -0.5853,  0.3285],\n",
       "         [-0.8136,  1.5261, -1.8419]],\n",
       "\n",
       "        [[ 0.4746,  0.6109,  2.3574],\n",
       "         [ 0.4926, -0.2901, -0.0702]],\n",
       "\n",
       "        [[-0.5164,  0.0244, -0.0032],\n",
       "         [ 0.5260, -1.3630,  0.6926]],\n",
       "\n",
       "        [[ 0.8845,  0.5479,  0.5138],\n",
       "         [-0.5159, -0.3496,  1.0330]],\n",
       "\n",
       "        [[ 1.4786, -0.2551,  0.9379],\n",
       "         [ 0.7103,  1.4898,  0.8830]],\n",
       "\n",
       "        [[-0.8963,  1.5017,  0.4556],\n",
       "         [ 0.3166, -0.4448,  0.1659]],\n",
       "\n",
       "        [[ 0.6303, -1.0407, -0.6283],\n",
       "         [ 0.6218, -0.0825,  0.2399]],\n",
       "\n",
       "        [[-0.7028,  0.5837, -0.8596],\n",
       "         [-0.2575, -1.3198, -0.0351]],\n",
       "\n",
       "        [[ 1.5892,  0.6892,  0.1056],\n",
       "         [-0.1241,  2.5676,  0.8052]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "input_size = 3\n",
    "hidden_size = 4 \n",
    "\n",
    "inputs = torch.randn(seq_len, batch_size, input_size)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hidden_0 = (h_0, c_0)\n",
    "hidden_0 = (torch.zeros(1, batch_size, hidden_size), torch.zeros(1, batch_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, lstm_hidden = lstm(inputs)\n",
    "lstm_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0959, -0.1320, -0.0067,  0.0662],\n",
       "         [ 0.0073, -0.0925, -0.0486,  0.0990]],\n",
       "\n",
       "        [[-0.0076, -0.2195, -0.0844,  0.1473],\n",
       "         [ 0.2227, -0.0923,  0.2469,  0.0069]],\n",
       "\n",
       "        [[ 0.1013, -0.5379, -0.1066,  0.0890],\n",
       "         [ 0.0693, -0.2018,  0.0484,  0.0711]],\n",
       "\n",
       "        [[ 0.1607, -0.3430, -0.0485,  0.0651],\n",
       "         [ 0.0159, -0.2670, -0.0493,  0.1449]],\n",
       "\n",
       "        [[ 0.0541, -0.4162, -0.1013,  0.0666],\n",
       "         [ 0.1495, -0.3597, -0.0442,  0.1110]],\n",
       "\n",
       "        [[-0.0312, -0.4489, -0.1356,  0.1399],\n",
       "         [ 0.1588, -0.4835, -0.0700,  0.0115]],\n",
       "\n",
       "        [[ 0.3148, -0.4199,  0.0726, -0.0018],\n",
       "         [ 0.0424, -0.3564, -0.0869,  0.0696]],\n",
       "\n",
       "        [[ 0.0155, -0.2154, -0.0288,  0.0884],\n",
       "         [ 0.0199, -0.3429, -0.1155,  0.0937]],\n",
       "\n",
       "        [[ 0.1788, -0.1908,  0.1507,  0.0107],\n",
       "         [ 0.0310, -0.2436, -0.0863,  0.1225]],\n",
       "\n",
       "        [[ 0.0113, -0.3244,  0.0020,  0.0845],\n",
       "         [ 0.3328, -0.4650,  0.0980, -0.0854]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to put hidden inputs to zeros, there is no need to provide them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0959, -0.1320, -0.0067,  0.0662],\n",
       "         [ 0.0073, -0.0925, -0.0486,  0.0990]],\n",
       "\n",
       "        [[-0.0076, -0.2195, -0.0844,  0.1473],\n",
       "         [ 0.2227, -0.0923,  0.2469,  0.0069]],\n",
       "\n",
       "        [[ 0.1013, -0.5379, -0.1066,  0.0890],\n",
       "         [ 0.0693, -0.2018,  0.0484,  0.0711]],\n",
       "\n",
       "        [[ 0.1607, -0.3430, -0.0485,  0.0651],\n",
       "         [ 0.0159, -0.2670, -0.0493,  0.1449]],\n",
       "\n",
       "        [[ 0.0541, -0.4162, -0.1013,  0.0666],\n",
       "         [ 0.1495, -0.3597, -0.0442,  0.1110]],\n",
       "\n",
       "        [[-0.0312, -0.4489, -0.1356,  0.1399],\n",
       "         [ 0.1588, -0.4835, -0.0700,  0.0115]],\n",
       "\n",
       "        [[ 0.3148, -0.4199,  0.0726, -0.0018],\n",
       "         [ 0.0424, -0.3564, -0.0869,  0.0696]],\n",
       "\n",
       "        [[ 0.0155, -0.2154, -0.0288,  0.0884],\n",
       "         [ 0.0199, -0.3429, -0.1155,  0.0937]],\n",
       "\n",
       "        [[ 0.1788, -0.1908,  0.1507,  0.0107],\n",
       "         [ 0.0310, -0.2436, -0.0863,  0.1225]],\n",
       "\n",
       "        [[ 0.0113, -0.3244,  0.0020,  0.0845],\n",
       "         [ 0.3328, -0.4650,  0.0980, -0.0854]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, lstm_hidden = lstm(inputs)\n",
    "lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the last output is the output of RRR. We can get it by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0113, -0.3244,  0.0020,  0.0845],\n",
       "        [ 0.3328, -0.4650,  0.0980, -0.0854]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often convient to have batches as the first dimension of the input. One can do it by adding `batch_first=True` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4995,  0.3639,  1.0023],\n",
       "         [ 1.0267, -0.0418, -1.1803],\n",
       "         [-1.2794,  0.5719, -0.9201],\n",
       "         [ 0.5916, -0.1258, -0.5609],\n",
       "         [ 0.5599,  1.4059, -2.3406],\n",
       "         [ 0.3924,  3.0559,  1.2309],\n",
       "         [ 1.4677, -0.8471, -0.8881],\n",
       "         [ 0.6779, -0.4732,  0.2142],\n",
       "         [-0.3178,  0.0881,  0.3831],\n",
       "         [-0.4190, -1.8207, -0.4366]],\n",
       "\n",
       "        [[ 0.5097, -0.3637,  2.2172],\n",
       "         [-0.0164,  2.0788,  0.6666],\n",
       "         [-0.0614, -0.8359,  0.3453],\n",
       "         [ 1.7959,  1.3562,  0.5197],\n",
       "         [-0.2985, -1.9809,  0.8117],\n",
       "         [-0.6387, -0.1323,  0.6102],\n",
       "         [-0.2652,  0.7346, -0.0485],\n",
       "         [-0.6944,  1.0064, -0.1709],\n",
       "         [-0.7343,  1.0906,  0.3458],\n",
       "         [ 1.1296,  0.8161,  0.1510]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_batch_first = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True) \n",
    "inputs_batch_first = torch.randn(batch_size, seq_len, input_size)\n",
    "inputs_batch_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0180,  0.0958, -0.0386, -0.0083],\n",
       "         [ 0.1155,  0.0670, -0.0143, -0.2921],\n",
       "         [ 0.2107,  0.0477, -0.0457, -0.0575],\n",
       "         [ 0.1591,  0.0707, -0.0224, -0.1556],\n",
       "         [ 0.1875, -0.0190, -0.0561, -0.3077],\n",
       "         [ 0.3834,  0.0082, -0.2214, -0.0712],\n",
       "         [ 0.1258,  0.0991,  0.0134, -0.3009],\n",
       "         [ 0.0858,  0.1459,  0.0551, -0.2215],\n",
       "         [ 0.0719,  0.1385, -0.0023, -0.0827],\n",
       "         [-0.0731,  0.2035,  0.0567, -0.1343]],\n",
       "\n",
       "        [[-0.1764,  0.1921,  0.0474,  0.0020],\n",
       "         [ 0.1407,  0.0510, -0.0922,  0.0498],\n",
       "         [-0.0446,  0.1757,  0.0133, -0.0043],\n",
       "         [ 0.0835,  0.0420, -0.0943, -0.1630],\n",
       "         [-0.1814,  0.2539,  0.0805, -0.0462],\n",
       "         [-0.0699,  0.1519,  0.0283, -0.0131],\n",
       "         [ 0.1240,  0.0779, -0.0393,  0.0159],\n",
       "         [ 0.2013,  0.0538, -0.0800,  0.1170],\n",
       "         [ 0.1948,  0.0586, -0.1122,  0.1770],\n",
       "         [ 0.1814,  0.0569, -0.1241, -0.0141]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, lstm_hidden = lstm_batch_first(inputs_batch_first)\n",
    "lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get the finial output by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0731,  0.2035,  0.0567, -0.1343],\n",
       "        [ 0.1814,  0.0569, -0.1241, -0.0141]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[39, 19, 89, 37, 56, 68, 20, 20, 92, 38],\n",
       "        [11, 41, 45, 99, 83, 54, 13, 41, 89, 43]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size = 100\n",
    "sentences = torch.randint(dict_size, (batch_size, seq_len))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 3\n",
    "embedding = nn.Embedding(dict_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8193,  0.7358,  0.8764],\n",
       "         [-1.2957,  0.2990,  0.6528],\n",
       "         [-0.4799, -0.6350, -0.1438],\n",
       "         [ 0.4397, -1.0167, -1.0566],\n",
       "         [-1.3190,  0.8336,  1.5785],\n",
       "         [-0.1942, -2.1960, -0.0441],\n",
       "         [ 0.6278, -0.5967, -0.7711],\n",
       "         [ 0.6278, -0.5967, -0.7711],\n",
       "         [ 0.1336,  2.0595,  1.3815],\n",
       "         [ 0.3533, -1.8378,  0.5807]],\n",
       "\n",
       "        [[ 0.0600,  1.0467, -0.2930],\n",
       "         [-0.5910, -0.1710, -1.3986],\n",
       "         [ 1.4898, -0.7162, -0.2831],\n",
       "         [ 0.0078, -0.2692, -0.5832],\n",
       "         [ 0.7588,  0.1370, -1.3416],\n",
       "         [-0.4860,  1.2992, -0.1855],\n",
       "         [-1.3901, -0.9563, -0.7431],\n",
       "         [-0.5910, -0.1710, -1.3986],\n",
       "         [-0.4799, -0.6350, -0.1438],\n",
       "         [ 1.2129,  0.1076,  1.1489]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_embedded = embedding(sentences)\n",
    "sentences_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0991,  0.2569,  0.0904, -0.1046],\n",
       "        [ 0.0167,  0.1325,  0.0105, -0.0944]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, _ = lstm_batch_first(sentences_embedded)\n",
    "lstm_out[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)\n",
    "\n",
    "Next we consider a dataset with text and the goal is to evaluate whether they are toxic or non-toxic.\n",
    "\n",
    "You can download the dataset in the following link:\n",
    "\n",
    "[here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments_df = pd.read_csv(\"data/jigsaw-toxic-comment-classification-challenge/train.csv\")[:10000]\n",
    "comments_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>your support for Chris Lawson \\n\\nAre you sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5578</th>\n",
       "      <td>Apologies for the above litany of bot-generate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text\n",
       "1911  your support for Chris Lawson \\n\\nAre you sure...\n",
       "5578  Apologies for the above litany of bot-generate..."
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "label_colnames = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(comments_df[['comment_text']], comments_df[label_colnames], random_state=667)\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z ]')\n",
    "STEMMER = SnowballStemmer('english')\n",
    "\n",
    "class TextPreprocessor:\n",
    "        \n",
    "    def transfrom_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(BAD_SYMBOLS_RE, \" \", text) # process bad symbols\n",
    "        # text = \" \".join([STEMMER.stem(word) for word in text.split()])\n",
    "        return text\n",
    "    \n",
    "    def transform(self, series):\n",
    "        return series.apply(lambda text: self.transfrom_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "X_train_preprocessed = preprocessor.transform(X_train['comment_text'])\n",
    "X_test_preprocessed = preprocessor.transform(X_test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your support for Chris Lawson \n",
      "\n",
      "Are you sure you want to support this guy?  Look at his source/quote for making the Red Baron jewish.  read the discussion page  on it. See the opposers views on his request page.  It scares me.   JohnHistory\n",
      "\n",
      "http://en.wikipedia.org/wiki/Talk:Manfred_von_Richthofen\n",
      "\n",
      "http://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship/Clawson\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "your support for chris lawson   are you sure you want to support this guy   look at his source quote for making the red baron jewish   read the discussion page  on it  see the opposers views on his request page   it scares me    johnhistory  http   en wikipedia org wiki talk manfred von richthofen  http   en wikipedia org wiki wikipedia requests for adminship clawson\n"
     ]
    }
   ],
   "source": [
    "print(X_train[\"comment_text\"].iloc[0])\n",
    "print('---------------------------------------------------------------------------------------------------------------------')\n",
    "print(X_train_preprocessed.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apologies for the above litany of bot-generated warnings and threats.  I think the bot needs its rules adjusted, and have notified the operator accordingly.   (call me Russ)\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "apologies for the above litany of bot generated warnings and threats   i think the bot needs its rules adjusted  and have notified the operator accordingly     call me russ \n"
     ]
    }
   ],
   "source": [
    "print(X_train[\"comment_text\"].iloc[1])\n",
    "print('---------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "print(X_train_preprocessed.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(text):\n",
    "    word_set = set()\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "    word_list = [\"<UNK>\", \"<PAD>\"] + sorted(list(word_set))\n",
    "    word2idx = {word_list[idx]: idx for idx in range(len(word_list))}\n",
    "    idx2word = {idx: word_list[idx] for idx in range(len(word_list))}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = None\n",
    "        self.idx2word = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        text = \" \".join(X)\n",
    "        self.word2idx, self.idx2word = create_dicts(text)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [self.transform_line(line) for line in X]\n",
    "        \n",
    "    def transform_line(self, line):\n",
    "        return [self.word2idx.get(word, 0) for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(X_train_preprocessed)\n",
    "tokenizer.fit(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenizer.transform(X_train_preprocessed)\n",
    "X_test_tokenized = tokenizer.transform(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutter:\n",
    "\n",
    "    def __init__(self, size=150):\n",
    "        self.size = size\n",
    "        \n",
    "    def transform(self, X):\n",
    "        new_X = []\n",
    "        for line in X:\n",
    "            new_line = line[:self.size]\n",
    "            new_line = new_line + [1] * (self.size - len(new_line))\n",
    "            new_X.append(new_line)\n",
    "        return new_X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = Cutter()\n",
    "X_train_cutted = cutter.transform(X_train_tokenized)\n",
    "X_test_cutted = cutter.transform(X_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.from_numpy(y_train.values)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.tensor(X_train_cutted), torch.from_numpy(y_train.values).float())\n",
    "test_data = TensorDataset(torch.tensor(X_test_cutted), torch.from_numpy(y_test.values).float())\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, dict_size, output_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(dict_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeded)\n",
    "        lstm_out = lstm_out[:, -1]        \n",
    "        logits = self.fc(lstm_out)\n",
    "        out = self.sigmoid(logits)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(tokenizer.word2idx)\n",
    "output_size = len(label_colnames)\n",
    "embedding_dim = 3\n",
    "hidden_dim = 4\n",
    "\n",
    "lstm_model = LSTMModel(dict_size, output_size, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7500, 150])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch = torch.tensor(X_train_cutted)\n",
    "X_test_torch = torch.tensor(X_test_cutted)\n",
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        ...,\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4603, 0.6068, 0.4962, 0.4396, 0.4761, 0.4731],\n",
       "        [0.4642, 0.6969, 0.5478, 0.3973, 0.5128, 0.5052]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model(X_train_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4642, 0.6969, 0.5478, 0.3973, 0.5128, 0.5052],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4645, 0.6594, 0.5345, 0.4379, 0.4969, 0.5013],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4374, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4449, 0.6412, 0.5177, 0.3787, 0.4731, 0.4768],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4375, 0.6426, 0.5188, 0.3793, 0.4714, 0.4830],\n",
       "        [0.4527, 0.6429, 0.5156, 0.4045, 0.4838, 0.4834]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "input_data, labels = dataiter.next()\n",
    "lstm_model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1, Avg. Loss: 0.17174386978149414\n",
      "Epoch: 1, Batch: 5, Avg. Loss: 0.6690165847539902\n",
      "Epoch: 1, Batch: 9, Avg. Loss: 0.641194224357605\n",
      "Epoch: 1, Batch: 13, Avg. Loss: 0.6107909381389618\n",
      "Epoch: 1, Batch: 17, Avg. Loss: 0.5849012285470963\n",
      "Epoch: 1, Batch: 21, Avg. Loss: 0.5561513751745224\n",
      "Epoch: 1, Batch: 25, Avg. Loss: 0.5274698883295059\n",
      "Epoch: 1, Batch: 29, Avg. Loss: 0.503442220389843\n",
      "Epoch: 1, Batch: 33, Avg. Loss: 0.4636116996407509\n",
      "Epoch: 1, Batch: 37, Avg. Loss: 0.44287507236003876\n",
      "Epoch: 1, Batch: 41, Avg. Loss: 0.4117263853549957\n",
      "Epoch: 1, Batch: 45, Avg. Loss: 0.3854990378022194\n",
      "Epoch: 1, Batch: 49, Avg. Loss: 0.3564751446247101\n",
      "Epoch: 1, Batch: 53, Avg. Loss: 0.36759430170059204\n",
      "Epoch: 1, Batch: 57, Avg. Loss: 0.31594623625278473\n",
      "Epoch: 1, Batch: 61, Avg. Loss: 0.2806461751461029\n",
      "Epoch: 1, Batch: 65, Avg. Loss: 0.26762253791093826\n",
      "Epoch: 1, Batch: 69, Avg. Loss: 0.29034635052084923\n",
      "Epoch: 1, Batch: 73, Avg. Loss: 0.26492880284786224\n",
      "Epoch: 1, Batch: 77, Avg. Loss: 0.2702476605772972\n",
      "Epoch: 1, Batch: 81, Avg. Loss: 0.2398987039923668\n",
      "Epoch: 1, Batch: 85, Avg. Loss: 0.20055421069264412\n",
      "Epoch: 1, Batch: 89, Avg. Loss: 0.2331908904016018\n",
      "Epoch: 1, Batch: 93, Avg. Loss: 0.21128998324275017\n",
      "Epoch: 1, Batch: 97, Avg. Loss: 0.23077736049890518\n",
      "Epoch: 1, Batch: 101, Avg. Loss: 0.21582939103245735\n",
      "Epoch: 1, Batch: 105, Avg. Loss: 0.20322594419121742\n",
      "Epoch: 1, Batch: 109, Avg. Loss: 0.15527375042438507\n",
      "Epoch: 1, Batch: 113, Avg. Loss: 0.24231091886758804\n",
      "Epoch: 1, Batch: 117, Avg. Loss: 0.19186526909470558\n",
      "Epoch: 1, Batch: 121, Avg. Loss: 0.2517739459872246\n",
      "Epoch: 1, Batch: 125, Avg. Loss: 0.19843417406082153\n",
      "Epoch: 1, Batch: 129, Avg. Loss: 0.16555747762322426\n",
      "Epoch: 1, Batch: 133, Avg. Loss: 0.19955286011099815\n",
      "Epoch: 1, Batch: 137, Avg. Loss: 0.13566632382571697\n",
      "Epoch: 1, Batch: 141, Avg. Loss: 0.1451149322092533\n",
      "Epoch: 1, Batch: 145, Avg. Loss: 0.2050975300371647\n",
      "Epoch: 1, Batch: 149, Avg. Loss: 0.22006001323461533\n",
      "Epoch: 1, Batch: 153, Avg. Loss: 0.15766014158725739\n",
      "Epoch: 1, Batch: 157, Avg. Loss: 0.16681691072881222\n",
      "Epoch: 1, Batch: 161, Avg. Loss: 0.16833765618503094\n",
      "Epoch: 1, Batch: 165, Avg. Loss: 0.15139037370681763\n",
      "Epoch: 1, Batch: 169, Avg. Loss: 0.14502584747970104\n",
      "Epoch: 1, Batch: 173, Avg. Loss: 0.1265923660248518\n",
      "Epoch: 1, Batch: 177, Avg. Loss: 0.18609150126576424\n",
      "Epoch: 1, Batch: 181, Avg. Loss: 0.14393769390881062\n",
      "Epoch: 1, Batch: 185, Avg. Loss: 0.12947765551507473\n",
      "Epoch: 1, Batch: 189, Avg. Loss: 0.14312560856342316\n",
      "Epoch: 1, Batch: 193, Avg. Loss: 0.1805821768939495\n",
      "Epoch: 1, Batch: 197, Avg. Loss: 0.2009271029382944\n",
      "Epoch: 1, Batch: 201, Avg. Loss: 0.13588700629770756\n",
      "Epoch: 1, Batch: 205, Avg. Loss: 0.12922378256917\n",
      "Epoch: 1, Batch: 209, Avg. Loss: 0.10562366433441639\n",
      "Epoch: 1, Batch: 213, Avg. Loss: 0.11927834153175354\n",
      "Epoch: 1, Batch: 217, Avg. Loss: 0.16436185501515865\n",
      "Epoch: 1, Batch: 221, Avg. Loss: 0.14124353136867285\n",
      "Epoch: 1, Batch: 225, Avg. Loss: 0.11922742053866386\n",
      "Epoch: 1, Batch: 229, Avg. Loss: 0.1896630898118019\n",
      "Epoch: 1, Batch: 233, Avg. Loss: 0.20896210335195065\n",
      "Epoch: 2, Batch: 1, Avg. Loss: 0.06546542793512344\n",
      "Epoch: 2, Batch: 5, Avg. Loss: 0.16129480302333832\n",
      "Epoch: 2, Batch: 9, Avg. Loss: 0.19799639657139778\n",
      "Epoch: 2, Batch: 13, Avg. Loss: 0.11704670544713736\n",
      "Epoch: 2, Batch: 17, Avg. Loss: 0.16206396371126175\n",
      "Epoch: 2, Batch: 21, Avg. Loss: 0.11634444817900658\n",
      "Epoch: 2, Batch: 25, Avg. Loss: 0.18796809390187263\n",
      "Epoch: 2, Batch: 29, Avg. Loss: 0.1823008693754673\n",
      "Epoch: 2, Batch: 33, Avg. Loss: 0.1726221526041627\n",
      "Epoch: 2, Batch: 37, Avg. Loss: 0.12852268479764462\n",
      "Epoch: 2, Batch: 41, Avg. Loss: 0.13780375756323338\n",
      "Epoch: 2, Batch: 45, Avg. Loss: 0.12116605415940285\n",
      "Epoch: 2, Batch: 49, Avg. Loss: 0.12403302639722824\n",
      "Epoch: 2, Batch: 53, Avg. Loss: 0.14608507975935936\n",
      "Epoch: 2, Batch: 57, Avg. Loss: 0.14945515058934689\n",
      "Epoch: 2, Batch: 61, Avg. Loss: 0.13771032355725765\n",
      "Epoch: 2, Batch: 65, Avg. Loss: 0.13593213260173798\n",
      "Epoch: 2, Batch: 69, Avg. Loss: 0.14675350300967693\n",
      "Epoch: 2, Batch: 73, Avg. Loss: 0.12119316682219505\n",
      "Epoch: 2, Batch: 77, Avg. Loss: 0.13027186319231987\n",
      "Epoch: 2, Batch: 81, Avg. Loss: 0.1803142260760069\n",
      "Epoch: 2, Batch: 85, Avg. Loss: 0.15535584464669228\n",
      "Epoch: 2, Batch: 89, Avg. Loss: 0.09464998729526997\n",
      "Epoch: 2, Batch: 93, Avg. Loss: 0.1022743433713913\n",
      "Epoch: 2, Batch: 97, Avg. Loss: 0.19739077985286713\n",
      "Epoch: 2, Batch: 101, Avg. Loss: 0.1932275127619505\n",
      "Epoch: 2, Batch: 105, Avg. Loss: 0.07267794664949179\n",
      "Epoch: 2, Batch: 109, Avg. Loss: 0.14658243395388126\n",
      "Epoch: 2, Batch: 113, Avg. Loss: 0.13666640780866146\n",
      "Epoch: 2, Batch: 117, Avg. Loss: 0.1509732324630022\n",
      "Epoch: 2, Batch: 121, Avg. Loss: 0.16893587075173855\n",
      "Epoch: 2, Batch: 125, Avg. Loss: 0.21199344098567963\n",
      "Epoch: 2, Batch: 129, Avg. Loss: 0.17769605107605457\n",
      "Epoch: 2, Batch: 133, Avg. Loss: 0.12180220056325197\n",
      "Epoch: 2, Batch: 137, Avg. Loss: 0.12603853549808264\n",
      "Epoch: 2, Batch: 141, Avg. Loss: 0.09373715985566378\n",
      "Epoch: 2, Batch: 145, Avg. Loss: 0.1542323175817728\n",
      "Epoch: 2, Batch: 149, Avg. Loss: 0.10326893627643585\n",
      "Epoch: 2, Batch: 153, Avg. Loss: 0.09436130616813898\n",
      "Epoch: 2, Batch: 157, Avg. Loss: 0.1463234107941389\n",
      "Epoch: 2, Batch: 161, Avg. Loss: 0.12142517231404781\n",
      "Epoch: 2, Batch: 165, Avg. Loss: 0.14475848339498043\n",
      "Epoch: 2, Batch: 169, Avg. Loss: 0.14784988015890121\n",
      "Epoch: 2, Batch: 173, Avg. Loss: 0.12241236492991447\n",
      "Epoch: 2, Batch: 177, Avg. Loss: 0.18574682716280222\n",
      "Epoch: 2, Batch: 181, Avg. Loss: 0.11741542629897594\n",
      "Epoch: 2, Batch: 185, Avg. Loss: 0.21296002343297005\n",
      "Epoch: 2, Batch: 189, Avg. Loss: 0.17411086335778236\n",
      "Epoch: 2, Batch: 193, Avg. Loss: 0.14519714377820492\n",
      "Epoch: 2, Batch: 197, Avg. Loss: 0.14876636676490307\n",
      "Epoch: 2, Batch: 201, Avg. Loss: 0.12702910043299198\n",
      "Epoch: 2, Batch: 205, Avg. Loss: 0.16312189772725105\n",
      "Epoch: 2, Batch: 209, Avg. Loss: 0.1533899214118719\n",
      "Epoch: 2, Batch: 213, Avg. Loss: 0.14363314025104046\n",
      "Epoch: 2, Batch: 217, Avg. Loss: 0.13576355017721653\n",
      "Epoch: 2, Batch: 221, Avg. Loss: 0.11787249892950058\n",
      "Epoch: 2, Batch: 225, Avg. Loss: 0.1625552922487259\n",
      "Epoch: 2, Batch: 229, Avg. Loss: 0.11845960840582848\n",
      "Epoch: 2, Batch: 233, Avg. Loss: 0.1270929640159011\n",
      "Epoch: 3, Batch: 1, Avg. Loss: 0.054569244384765625\n",
      "Epoch: 3, Batch: 5, Avg. Loss: 0.14627834595739841\n",
      "Epoch: 3, Batch: 9, Avg. Loss: 0.1848635971546173\n",
      "Epoch: 3, Batch: 13, Avg. Loss: 0.22833281755447388\n",
      "Epoch: 3, Batch: 17, Avg. Loss: 0.1488742884248495\n",
      "Epoch: 3, Batch: 21, Avg. Loss: 0.16807929053902626\n",
      "Epoch: 3, Batch: 25, Avg. Loss: 0.12058888282626867\n",
      "Epoch: 3, Batch: 29, Avg. Loss: 0.18146571703255177\n",
      "Epoch: 3, Batch: 33, Avg. Loss: 0.10449419170618057\n",
      "Epoch: 3, Batch: 37, Avg. Loss: 0.14566432870924473\n",
      "Epoch: 3, Batch: 41, Avg. Loss: 0.1311145108193159\n",
      "Epoch: 3, Batch: 45, Avg. Loss: 0.0813875887542963\n",
      "Epoch: 3, Batch: 49, Avg. Loss: 0.1745187658816576\n",
      "Epoch: 3, Batch: 53, Avg. Loss: 0.187958100810647\n",
      "Epoch: 3, Batch: 57, Avg. Loss: 0.09456810541450977\n",
      "Epoch: 3, Batch: 61, Avg. Loss: 0.16064253821969032\n",
      "Epoch: 3, Batch: 65, Avg. Loss: 0.1308756461367011\n",
      "Epoch: 3, Batch: 69, Avg. Loss: 0.12844902090728283\n",
      "Epoch: 3, Batch: 73, Avg. Loss: 0.1597721241414547\n",
      "Epoch: 3, Batch: 77, Avg. Loss: 0.09380284231156111\n",
      "Epoch: 3, Batch: 81, Avg. Loss: 0.09672287199646235\n",
      "Epoch: 3, Batch: 85, Avg. Loss: 0.142755426466465\n",
      "Epoch: 3, Batch: 89, Avg. Loss: 0.10880728252232075\n",
      "Epoch: 3, Batch: 93, Avg. Loss: 0.15441208891570568\n",
      "Epoch: 3, Batch: 97, Avg. Loss: 0.0867361519485712\n",
      "Epoch: 3, Batch: 101, Avg. Loss: 0.12711666710674763\n",
      "Epoch: 3, Batch: 105, Avg. Loss: 0.11095566768199205\n",
      "Epoch: 3, Batch: 109, Avg. Loss: 0.11486047320067883\n",
      "Epoch: 3, Batch: 113, Avg. Loss: 0.19465395994484425\n",
      "Epoch: 3, Batch: 117, Avg. Loss: 0.20209761150181293\n",
      "Epoch: 3, Batch: 121, Avg. Loss: 0.14315742626786232\n",
      "Epoch: 3, Batch: 125, Avg. Loss: 0.1527441032230854\n",
      "Epoch: 3, Batch: 129, Avg. Loss: 0.16778110340237617\n",
      "Epoch: 3, Batch: 133, Avg. Loss: 0.09006315376609564\n",
      "Epoch: 3, Batch: 137, Avg. Loss: 0.09999101795256138\n",
      "Epoch: 3, Batch: 141, Avg. Loss: 0.14338020235300064\n",
      "Epoch: 3, Batch: 145, Avg. Loss: 0.1616194024682045\n",
      "Epoch: 3, Batch: 149, Avg. Loss: 0.13647885248064995\n",
      "Epoch: 3, Batch: 153, Avg. Loss: 0.13336795382201672\n",
      "Epoch: 3, Batch: 157, Avg. Loss: 0.11752838175743818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 161, Avg. Loss: 0.15669503808021545\n",
      "Epoch: 3, Batch: 165, Avg. Loss: 0.1445703748613596\n",
      "Epoch: 3, Batch: 169, Avg. Loss: 0.08340555336326361\n",
      "Epoch: 3, Batch: 173, Avg. Loss: 0.11521376390010118\n",
      "Epoch: 3, Batch: 177, Avg. Loss: 0.1515506524592638\n",
      "Epoch: 3, Batch: 181, Avg. Loss: 0.11082752794027328\n",
      "Epoch: 3, Batch: 185, Avg. Loss: 0.1779741309583187\n",
      "Epoch: 3, Batch: 189, Avg. Loss: 0.15236392430961132\n",
      "Epoch: 3, Batch: 193, Avg. Loss: 0.19292431324720383\n",
      "Epoch: 3, Batch: 197, Avg. Loss: 0.07736148498952389\n",
      "Epoch: 3, Batch: 201, Avg. Loss: 0.1265016458928585\n",
      "Epoch: 3, Batch: 205, Avg. Loss: 0.18320486694574356\n",
      "Epoch: 3, Batch: 209, Avg. Loss: 0.13840071111917496\n",
      "Epoch: 3, Batch: 213, Avg. Loss: 0.10890628304332495\n",
      "Epoch: 3, Batch: 217, Avg. Loss: 0.12891599722206593\n",
      "Epoch: 3, Batch: 221, Avg. Loss: 0.2146926000714302\n",
      "Epoch: 3, Batch: 225, Avg. Loss: 0.17347349226474762\n",
      "Epoch: 3, Batch: 229, Avg. Loss: 0.17317874170839787\n",
      "Epoch: 3, Batch: 233, Avg. Loss: 0.1558239944279194\n",
      "Epoch: 4, Batch: 1, Avg. Loss: 0.036642950028181076\n",
      "Epoch: 4, Batch: 5, Avg. Loss: 0.1092864666134119\n",
      "Epoch: 4, Batch: 9, Avg. Loss: 0.16328759118914604\n",
      "Epoch: 4, Batch: 13, Avg. Loss: 0.18301074393093586\n",
      "Epoch: 4, Batch: 17, Avg. Loss: 0.16261644661426544\n",
      "Epoch: 4, Batch: 21, Avg. Loss: 0.12593930773437023\n",
      "Epoch: 4, Batch: 25, Avg. Loss: 0.10777139477431774\n",
      "Epoch: 4, Batch: 29, Avg. Loss: 0.1510616634041071\n",
      "Epoch: 4, Batch: 33, Avg. Loss: 0.13100768812000751\n",
      "Epoch: 4, Batch: 37, Avg. Loss: 0.15024608001112938\n",
      "Epoch: 4, Batch: 41, Avg. Loss: 0.08423491194844246\n",
      "Epoch: 4, Batch: 45, Avg. Loss: 0.12707815319299698\n",
      "Epoch: 4, Batch: 49, Avg. Loss: 0.13164211250841618\n",
      "Epoch: 4, Batch: 53, Avg. Loss: 0.11683125328272581\n",
      "Epoch: 4, Batch: 57, Avg. Loss: 0.146542489528656\n",
      "Epoch: 4, Batch: 61, Avg. Loss: 0.1316745625808835\n",
      "Epoch: 4, Batch: 65, Avg. Loss: 0.10053417086601257\n",
      "Epoch: 4, Batch: 69, Avg. Loss: 0.20491481758654118\n",
      "Epoch: 4, Batch: 73, Avg. Loss: 0.1523146592080593\n",
      "Epoch: 4, Batch: 77, Avg. Loss: 0.10776042565703392\n",
      "Epoch: 4, Batch: 81, Avg. Loss: 0.11220429558306932\n",
      "Epoch: 4, Batch: 85, Avg. Loss: 0.1401715213432908\n",
      "Epoch: 4, Batch: 89, Avg. Loss: 0.14501845836639404\n",
      "Epoch: 4, Batch: 93, Avg. Loss: 0.20931158401072025\n",
      "Epoch: 4, Batch: 97, Avg. Loss: 0.16778493858873844\n",
      "Epoch: 4, Batch: 101, Avg. Loss: 0.12055125460028648\n",
      "Epoch: 4, Batch: 105, Avg. Loss: 0.14942893385887146\n",
      "Epoch: 4, Batch: 109, Avg. Loss: 0.16184063255786896\n",
      "Epoch: 4, Batch: 113, Avg. Loss: 0.14721239171922207\n",
      "Epoch: 4, Batch: 117, Avg. Loss: 0.07792959362268448\n",
      "Epoch: 4, Batch: 121, Avg. Loss: 0.1352676022797823\n",
      "Epoch: 4, Batch: 125, Avg. Loss: 0.11108726356178522\n",
      "Epoch: 4, Batch: 129, Avg. Loss: 0.15973537135869265\n",
      "Epoch: 4, Batch: 133, Avg. Loss: 0.11454859096556902\n",
      "Epoch: 4, Batch: 137, Avg. Loss: 0.11424518376588821\n",
      "Epoch: 4, Batch: 141, Avg. Loss: 0.11246210150420666\n",
      "Epoch: 4, Batch: 145, Avg. Loss: 0.17024687118828297\n",
      "Epoch: 4, Batch: 149, Avg. Loss: 0.11218366585671902\n",
      "Epoch: 4, Batch: 153, Avg. Loss: 0.0859554698690772\n",
      "Epoch: 4, Batch: 157, Avg. Loss: 0.22421259246766567\n",
      "Epoch: 4, Batch: 161, Avg. Loss: 0.12234719377011061\n",
      "Epoch: 4, Batch: 165, Avg. Loss: 0.17796857375651598\n",
      "Epoch: 4, Batch: 169, Avg. Loss: 0.0933873150497675\n",
      "Epoch: 4, Batch: 173, Avg. Loss: 0.11343502998352051\n",
      "Epoch: 4, Batch: 177, Avg. Loss: 0.14354863204061985\n",
      "Epoch: 4, Batch: 181, Avg. Loss: 0.208282720297575\n",
      "Epoch: 4, Batch: 185, Avg. Loss: 0.1834008488804102\n",
      "Epoch: 4, Batch: 189, Avg. Loss: 0.17585155181586742\n",
      "Epoch: 4, Batch: 193, Avg. Loss: 0.16808235459029675\n",
      "Epoch: 4, Batch: 197, Avg. Loss: 0.13943910598754883\n",
      "Epoch: 4, Batch: 201, Avg. Loss: 0.1660071462392807\n",
      "Epoch: 4, Batch: 205, Avg. Loss: 0.10048863850533962\n",
      "Epoch: 4, Batch: 209, Avg. Loss: 0.1547914557158947\n",
      "Epoch: 4, Batch: 213, Avg. Loss: 0.13960736617445946\n",
      "Epoch: 4, Batch: 217, Avg. Loss: 0.10124059859663248\n",
      "Epoch: 4, Batch: 221, Avg. Loss: 0.1431406270712614\n",
      "Epoch: 4, Batch: 225, Avg. Loss: 0.127370685338974\n",
      "Epoch: 4, Batch: 229, Avg. Loss: 0.12326835468411446\n",
      "Epoch: 4, Batch: 233, Avg. Loss: 0.24373874999582767\n",
      "Epoch: 5, Batch: 1, Avg. Loss: 0.020270423963665962\n",
      "Epoch: 5, Batch: 5, Avg. Loss: 0.10801723413169384\n",
      "Epoch: 5, Batch: 9, Avg. Loss: 0.19047927297651768\n",
      "Epoch: 5, Batch: 13, Avg. Loss: 0.13274052366614342\n",
      "Epoch: 5, Batch: 17, Avg. Loss: 0.16573872603476048\n",
      "Epoch: 5, Batch: 21, Avg. Loss: 0.14754087291657925\n",
      "Epoch: 5, Batch: 25, Avg. Loss: 0.20601468533277512\n",
      "Epoch: 5, Batch: 29, Avg. Loss: 0.08639672212302685\n",
      "Epoch: 5, Batch: 33, Avg. Loss: 0.14470824599266052\n",
      "Epoch: 5, Batch: 37, Avg. Loss: 0.1540139690041542\n",
      "Epoch: 5, Batch: 41, Avg. Loss: 0.11879215203225613\n",
      "Epoch: 5, Batch: 45, Avg. Loss: 0.10222495906054974\n",
      "Epoch: 5, Batch: 49, Avg. Loss: 0.1364029236137867\n",
      "Epoch: 5, Batch: 53, Avg. Loss: 0.10079317633062601\n",
      "Epoch: 5, Batch: 57, Avg. Loss: 0.14609168283641338\n",
      "Epoch: 5, Batch: 61, Avg. Loss: 0.12059395015239716\n",
      "Epoch: 5, Batch: 65, Avg. Loss: 0.08137180656194687\n",
      "Epoch: 5, Batch: 69, Avg. Loss: 0.13484470546245575\n",
      "Epoch: 5, Batch: 73, Avg. Loss: 0.16421856544911861\n",
      "Epoch: 5, Batch: 77, Avg. Loss: 0.1853439025580883\n",
      "Epoch: 5, Batch: 81, Avg. Loss: 0.20259892754256725\n",
      "Epoch: 5, Batch: 85, Avg. Loss: 0.12542646750807762\n",
      "Epoch: 5, Batch: 89, Avg. Loss: 0.13575973734259605\n",
      "Epoch: 5, Batch: 93, Avg. Loss: 0.11835990753024817\n",
      "Epoch: 5, Batch: 97, Avg. Loss: 0.12902649492025375\n",
      "Epoch: 5, Batch: 101, Avg. Loss: 0.08525862824171782\n",
      "Epoch: 5, Batch: 105, Avg. Loss: 0.08757037296891212\n",
      "Epoch: 5, Batch: 109, Avg. Loss: 0.11194661073386669\n",
      "Epoch: 5, Batch: 113, Avg. Loss: 0.1886126957833767\n",
      "Epoch: 5, Batch: 117, Avg. Loss: 0.15524685010313988\n",
      "Epoch: 5, Batch: 121, Avg. Loss: 0.1107357507571578\n",
      "Epoch: 5, Batch: 125, Avg. Loss: 0.13183761946856976\n",
      "Epoch: 5, Batch: 129, Avg. Loss: 0.18359499890357256\n",
      "Epoch: 5, Batch: 133, Avg. Loss: 0.12779787369072437\n",
      "Epoch: 5, Batch: 137, Avg. Loss: 0.16972551308572292\n",
      "Epoch: 5, Batch: 141, Avg. Loss: 0.11501588299870491\n",
      "Epoch: 5, Batch: 145, Avg. Loss: 0.18135277926921844\n",
      "Epoch: 5, Batch: 149, Avg. Loss: 0.18536362890154123\n",
      "Epoch: 5, Batch: 153, Avg. Loss: 0.11862011440098286\n",
      "Epoch: 5, Batch: 157, Avg. Loss: 0.13738269079476595\n",
      "Epoch: 5, Batch: 161, Avg. Loss: 0.11057661660015583\n",
      "Epoch: 5, Batch: 165, Avg. Loss: 0.13134284876286983\n",
      "Epoch: 5, Batch: 169, Avg. Loss: 0.14418472442775965\n",
      "Epoch: 5, Batch: 173, Avg. Loss: 0.17071504704654217\n",
      "Epoch: 5, Batch: 177, Avg. Loss: 0.17364300042390823\n",
      "Epoch: 5, Batch: 181, Avg. Loss: 0.12112963944673538\n",
      "Epoch: 5, Batch: 185, Avg. Loss: 0.105359829030931\n",
      "Epoch: 5, Batch: 189, Avg. Loss: 0.14268361032009125\n",
      "Epoch: 5, Batch: 193, Avg. Loss: 0.15646361745893955\n",
      "Epoch: 5, Batch: 197, Avg. Loss: 0.18512986786663532\n",
      "Epoch: 5, Batch: 201, Avg. Loss: 0.11707464139908552\n",
      "Epoch: 5, Batch: 205, Avg. Loss: 0.1574829239398241\n",
      "Epoch: 5, Batch: 209, Avg. Loss: 0.15692833997309208\n",
      "Epoch: 5, Batch: 213, Avg. Loss: 0.16222410835325718\n",
      "Epoch: 5, Batch: 217, Avg. Loss: 0.1423084530979395\n",
      "Epoch: 5, Batch: 221, Avg. Loss: 0.1902512162923813\n",
      "Epoch: 5, Batch: 225, Avg. Loss: 0.09716950170695782\n",
      "Epoch: 5, Batch: 229, Avg. Loss: 0.10812186170369387\n",
      "Epoch: 5, Batch: 233, Avg. Loss: 0.11903652176260948\n",
      "Epoch: 6, Batch: 1, Avg. Loss: 0.019138330593705177\n",
      "Epoch: 6, Batch: 5, Avg. Loss: 0.14141283929347992\n",
      "Epoch: 6, Batch: 9, Avg. Loss: 0.1143442178145051\n",
      "Epoch: 6, Batch: 13, Avg. Loss: 0.1615373557433486\n",
      "Epoch: 6, Batch: 17, Avg. Loss: 0.1241468396037817\n",
      "Epoch: 6, Batch: 21, Avg. Loss: 0.13855162262916565\n",
      "Epoch: 6, Batch: 25, Avg. Loss: 0.09525651764124632\n",
      "Epoch: 6, Batch: 29, Avg. Loss: 0.15072517190128565\n",
      "Epoch: 6, Batch: 33, Avg. Loss: 0.15185244847089052\n",
      "Epoch: 6, Batch: 37, Avg. Loss: 0.19771159812808037\n",
      "Epoch: 6, Batch: 41, Avg. Loss: 0.10820421203970909\n",
      "Epoch: 6, Batch: 45, Avg. Loss: 0.16182316560298204\n",
      "Epoch: 6, Batch: 49, Avg. Loss: 0.18859782628715038\n",
      "Epoch: 6, Batch: 53, Avg. Loss: 0.20464776270091534\n",
      "Epoch: 6, Batch: 57, Avg. Loss: 0.1727349953725934\n",
      "Epoch: 6, Batch: 61, Avg. Loss: 0.09301161766052246\n",
      "Epoch: 6, Batch: 65, Avg. Loss: 0.14397599548101425\n",
      "Epoch: 6, Batch: 69, Avg. Loss: 0.18729495257139206\n",
      "Epoch: 6, Batch: 73, Avg. Loss: 0.15722916927188635\n",
      "Epoch: 6, Batch: 77, Avg. Loss: 0.22882768139243126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Batch: 81, Avg. Loss: 0.14100275188684464\n",
      "Epoch: 6, Batch: 85, Avg. Loss: 0.17161220498383045\n",
      "Epoch: 6, Batch: 89, Avg. Loss: 0.12877737917006016\n",
      "Epoch: 6, Batch: 93, Avg. Loss: 0.10584971494972706\n",
      "Epoch: 6, Batch: 97, Avg. Loss: 0.18328346125781536\n",
      "Epoch: 6, Batch: 101, Avg. Loss: 0.1612074263393879\n",
      "Epoch: 6, Batch: 105, Avg. Loss: 0.13530327007174492\n",
      "Epoch: 6, Batch: 109, Avg. Loss: 0.12502680998295546\n",
      "Epoch: 6, Batch: 113, Avg. Loss: 0.15301249735057354\n",
      "Epoch: 6, Batch: 117, Avg. Loss: 0.09920753166079521\n",
      "Epoch: 6, Batch: 121, Avg. Loss: 0.11094674654304981\n",
      "Epoch: 6, Batch: 125, Avg. Loss: 0.13274701219052076\n",
      "Epoch: 6, Batch: 129, Avg. Loss: 0.14075769297778606\n",
      "Epoch: 6, Batch: 133, Avg. Loss: 0.08336008712649345\n",
      "Epoch: 6, Batch: 137, Avg. Loss: 0.15467119589447975\n",
      "Epoch: 6, Batch: 141, Avg. Loss: 0.19701935723423958\n",
      "Epoch: 6, Batch: 145, Avg. Loss: 0.14743906073272228\n",
      "Epoch: 6, Batch: 149, Avg. Loss: 0.16562780179083347\n",
      "Epoch: 6, Batch: 153, Avg. Loss: 0.1515119131654501\n",
      "Epoch: 6, Batch: 157, Avg. Loss: 0.16096513904631138\n",
      "Epoch: 6, Batch: 161, Avg. Loss: 0.11830918025225401\n",
      "Epoch: 6, Batch: 165, Avg. Loss: 0.11506713554263115\n",
      "Epoch: 6, Batch: 169, Avg. Loss: 0.1476433426141739\n",
      "Epoch: 6, Batch: 173, Avg. Loss: 0.1309287864714861\n",
      "Epoch: 6, Batch: 177, Avg. Loss: 0.11221002694219351\n",
      "Epoch: 6, Batch: 181, Avg. Loss: 0.10416611284017563\n",
      "Epoch: 6, Batch: 185, Avg. Loss: 0.14084517769515514\n",
      "Epoch: 6, Batch: 189, Avg. Loss: 0.11766389943659306\n",
      "Epoch: 6, Batch: 193, Avg. Loss: 0.11596012208610773\n",
      "Epoch: 6, Batch: 197, Avg. Loss: 0.11558223608881235\n",
      "Epoch: 6, Batch: 201, Avg. Loss: 0.12120221182703972\n",
      "Epoch: 6, Batch: 205, Avg. Loss: 0.09241313673555851\n",
      "Epoch: 6, Batch: 209, Avg. Loss: 0.1514128763228655\n",
      "Epoch: 6, Batch: 213, Avg. Loss: 0.10237843915820122\n",
      "Epoch: 6, Batch: 217, Avg. Loss: 0.11033435724675655\n",
      "Epoch: 6, Batch: 221, Avg. Loss: 0.12701768893748522\n",
      "Epoch: 6, Batch: 225, Avg. Loss: 0.1359731573611498\n",
      "Epoch: 6, Batch: 229, Avg. Loss: 0.11391793005168438\n",
      "Epoch: 6, Batch: 233, Avg. Loss: 0.12555064633488655\n",
      "Epoch: 7, Batch: 1, Avg. Loss: 0.038123030215501785\n",
      "Epoch: 7, Batch: 5, Avg. Loss: 0.12292984686791897\n",
      "Epoch: 7, Batch: 9, Avg. Loss: 0.15153124555945396\n",
      "Epoch: 7, Batch: 13, Avg. Loss: 0.16561907157301903\n",
      "Epoch: 7, Batch: 17, Avg. Loss: 0.11841532308608294\n",
      "Epoch: 7, Batch: 21, Avg. Loss: 0.13631027564406395\n",
      "Epoch: 7, Batch: 25, Avg. Loss: 0.11763903126120567\n",
      "Epoch: 7, Batch: 29, Avg. Loss: 0.13992485404014587\n",
      "Epoch: 7, Batch: 33, Avg. Loss: 0.13517951592803001\n",
      "Epoch: 7, Batch: 37, Avg. Loss: 0.09095617197453976\n",
      "Epoch: 7, Batch: 41, Avg. Loss: 0.23925811424851418\n",
      "Epoch: 7, Batch: 45, Avg. Loss: 0.15924097038805485\n",
      "Epoch: 7, Batch: 49, Avg. Loss: 0.14737901091575623\n",
      "Epoch: 7, Batch: 53, Avg. Loss: 0.157887639477849\n",
      "Epoch: 7, Batch: 57, Avg. Loss: 0.15213035233318806\n",
      "Epoch: 7, Batch: 61, Avg. Loss: 0.08279765490442514\n",
      "Epoch: 7, Batch: 65, Avg. Loss: 0.1460837945342064\n",
      "Epoch: 7, Batch: 69, Avg. Loss: 0.16793113388121128\n",
      "Epoch: 7, Batch: 73, Avg. Loss: 0.12025511637330055\n",
      "Epoch: 7, Batch: 77, Avg. Loss: 0.125892523676157\n",
      "Epoch: 7, Batch: 81, Avg. Loss: 0.08778302930295467\n",
      "Epoch: 7, Batch: 85, Avg. Loss: 0.15210534073412418\n",
      "Epoch: 7, Batch: 89, Avg. Loss: 0.23833612352609634\n",
      "Epoch: 7, Batch: 93, Avg. Loss: 0.1231484841555357\n",
      "Epoch: 7, Batch: 97, Avg. Loss: 0.13902699388563633\n",
      "Epoch: 7, Batch: 101, Avg. Loss: 0.11598226986825466\n",
      "Epoch: 7, Batch: 105, Avg. Loss: 0.14328709430992603\n",
      "Epoch: 7, Batch: 109, Avg. Loss: 0.1033752104267478\n",
      "Epoch: 7, Batch: 113, Avg. Loss: 0.14653644524514675\n",
      "Epoch: 7, Batch: 117, Avg. Loss: 0.08251480478793383\n",
      "Epoch: 7, Batch: 121, Avg. Loss: 0.13713971339166164\n",
      "Epoch: 7, Batch: 125, Avg. Loss: 0.14512175507843494\n",
      "Epoch: 7, Batch: 129, Avg. Loss: 0.08908334281295538\n",
      "Epoch: 7, Batch: 133, Avg. Loss: 0.2334551364183426\n",
      "Epoch: 7, Batch: 137, Avg. Loss: 0.11030531767755747\n",
      "Epoch: 7, Batch: 141, Avg. Loss: 0.08411183580756187\n",
      "Epoch: 7, Batch: 145, Avg. Loss: 0.16990498639643192\n",
      "Epoch: 7, Batch: 149, Avg. Loss: 0.09341830853372812\n",
      "Epoch: 7, Batch: 153, Avg. Loss: 0.1516752503812313\n",
      "Epoch: 7, Batch: 157, Avg. Loss: 0.15846595354378223\n",
      "Epoch: 7, Batch: 161, Avg. Loss: 0.15006636828184128\n",
      "Epoch: 7, Batch: 165, Avg. Loss: 0.10942652635276318\n",
      "Epoch: 7, Batch: 169, Avg. Loss: 0.1083988081663847\n",
      "Epoch: 7, Batch: 173, Avg. Loss: 0.09246544260531664\n",
      "Epoch: 7, Batch: 177, Avg. Loss: 0.16751946695148945\n",
      "Epoch: 7, Batch: 181, Avg. Loss: 0.10780472680926323\n",
      "Epoch: 7, Batch: 185, Avg. Loss: 0.14479960314929485\n",
      "Epoch: 7, Batch: 189, Avg. Loss: 0.14008495397865772\n",
      "Epoch: 7, Batch: 193, Avg. Loss: 0.139949519187212\n",
      "Epoch: 7, Batch: 197, Avg. Loss: 0.1340817790478468\n",
      "Epoch: 7, Batch: 201, Avg. Loss: 0.16993549652397633\n",
      "Epoch: 7, Batch: 205, Avg. Loss: 0.14509993977844715\n",
      "Epoch: 7, Batch: 209, Avg. Loss: 0.1637770663946867\n",
      "Epoch: 7, Batch: 213, Avg. Loss: 0.13431188184767962\n",
      "Epoch: 7, Batch: 217, Avg. Loss: 0.1357580404728651\n",
      "Epoch: 7, Batch: 221, Avg. Loss: 0.14465571474283934\n",
      "Epoch: 7, Batch: 225, Avg. Loss: 0.18685774132609367\n",
      "Epoch: 7, Batch: 229, Avg. Loss: 0.15471820905804634\n",
      "Epoch: 7, Batch: 233, Avg. Loss: 0.1287493845447898\n",
      "Epoch: 8, Batch: 1, Avg. Loss: 0.02676619030535221\n",
      "Epoch: 8, Batch: 5, Avg. Loss: 0.1244270820170641\n",
      "Epoch: 8, Batch: 9, Avg. Loss: 0.15323277562856674\n",
      "Epoch: 8, Batch: 13, Avg. Loss: 0.15850875433534384\n",
      "Epoch: 8, Batch: 17, Avg. Loss: 0.10440128669142723\n",
      "Epoch: 8, Batch: 21, Avg. Loss: 0.107964426279068\n",
      "Epoch: 8, Batch: 25, Avg. Loss: 0.11376640386879444\n",
      "Epoch: 8, Batch: 29, Avg. Loss: 0.17092795949429274\n",
      "Epoch: 8, Batch: 33, Avg. Loss: 0.15802018158137798\n",
      "Epoch: 8, Batch: 37, Avg. Loss: 0.11849496234208345\n",
      "Epoch: 8, Batch: 41, Avg. Loss: 0.12977717630565166\n",
      "Epoch: 8, Batch: 45, Avg. Loss: 0.13539567310363054\n",
      "Epoch: 8, Batch: 49, Avg. Loss: 0.15198058634996414\n",
      "Epoch: 8, Batch: 53, Avg. Loss: 0.1446649171411991\n",
      "Epoch: 8, Batch: 57, Avg. Loss: 0.11207144986838102\n",
      "Epoch: 8, Batch: 61, Avg. Loss: 0.21335401013493538\n",
      "Epoch: 8, Batch: 65, Avg. Loss: 0.11903473921120167\n",
      "Epoch: 8, Batch: 69, Avg. Loss: 0.10663812980055809\n",
      "Epoch: 8, Batch: 73, Avg. Loss: 0.16149942763149738\n",
      "Epoch: 8, Batch: 77, Avg. Loss: 0.14297494012862444\n",
      "Epoch: 8, Batch: 81, Avg. Loss: 0.07389718201011419\n",
      "Epoch: 8, Batch: 85, Avg. Loss: 0.1114113200455904\n",
      "Epoch: 8, Batch: 89, Avg. Loss: 0.12721901759505272\n",
      "Epoch: 8, Batch: 93, Avg. Loss: 0.1345428228378296\n",
      "Epoch: 8, Batch: 97, Avg. Loss: 0.14737178198993206\n",
      "Epoch: 8, Batch: 101, Avg. Loss: 0.12046412099152803\n",
      "Epoch: 8, Batch: 105, Avg. Loss: 0.16038256511092186\n",
      "Epoch: 8, Batch: 109, Avg. Loss: 0.14234457723796368\n",
      "Epoch: 8, Batch: 113, Avg. Loss: 0.15469266660511494\n",
      "Epoch: 8, Batch: 117, Avg. Loss: 0.07221932336688042\n",
      "Epoch: 8, Batch: 121, Avg. Loss: 0.14388916082680225\n",
      "Epoch: 8, Batch: 125, Avg. Loss: 0.055984653532505035\n",
      "Epoch: 8, Batch: 129, Avg. Loss: 0.1772857904434204\n",
      "Epoch: 8, Batch: 133, Avg. Loss: 0.16623679921030998\n",
      "Epoch: 8, Batch: 137, Avg. Loss: 0.23043284192681313\n",
      "Epoch: 8, Batch: 141, Avg. Loss: 0.1689338255673647\n",
      "Epoch: 8, Batch: 145, Avg. Loss: 0.09964406490325928\n",
      "Epoch: 8, Batch: 149, Avg. Loss: 0.12543033994734287\n",
      "Epoch: 8, Batch: 153, Avg. Loss: 0.13889224641025066\n",
      "Epoch: 8, Batch: 157, Avg. Loss: 0.11285320669412613\n",
      "Epoch: 8, Batch: 161, Avg. Loss: 0.09761663805693388\n",
      "Epoch: 8, Batch: 165, Avg. Loss: 0.1268436200916767\n",
      "Epoch: 8, Batch: 169, Avg. Loss: 0.09060042444616556\n",
      "Epoch: 8, Batch: 173, Avg. Loss: 0.16643145494163036\n",
      "Epoch: 8, Batch: 177, Avg. Loss: 0.1762958187609911\n",
      "Epoch: 8, Batch: 181, Avg. Loss: 0.13710477575659752\n",
      "Epoch: 8, Batch: 185, Avg. Loss: 0.12042211182415485\n",
      "Epoch: 8, Batch: 189, Avg. Loss: 0.17137060686945915\n",
      "Epoch: 8, Batch: 193, Avg. Loss: 0.13920248579233885\n",
      "Epoch: 8, Batch: 197, Avg. Loss: 0.17203917540609837\n",
      "Epoch: 8, Batch: 201, Avg. Loss: 0.11154437065124512\n",
      "Epoch: 8, Batch: 205, Avg. Loss: 0.13199488632380962\n",
      "Epoch: 8, Batch: 209, Avg. Loss: 0.1302708014845848\n",
      "Epoch: 8, Batch: 213, Avg. Loss: 0.19015852734446526\n",
      "Epoch: 8, Batch: 217, Avg. Loss: 0.16840370558202267\n",
      "Epoch: 8, Batch: 221, Avg. Loss: 0.139785498380661\n",
      "Epoch: 8, Batch: 225, Avg. Loss: 0.1550197582691908\n",
      "Epoch: 8, Batch: 229, Avg. Loss: 0.08144818898290396\n",
      "Epoch: 8, Batch: 233, Avg. Loss: 0.1783637274056673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Batch: 1, Avg. Loss: 0.042895060032606125\n",
      "Epoch: 9, Batch: 5, Avg. Loss: 0.13024031557142735\n",
      "Epoch: 9, Batch: 9, Avg. Loss: 0.14284091256558895\n",
      "Epoch: 9, Batch: 13, Avg. Loss: 0.15010309219360352\n",
      "Epoch: 9, Batch: 17, Avg. Loss: 0.13128844648599625\n",
      "Epoch: 9, Batch: 21, Avg. Loss: 0.10686303675174713\n",
      "Epoch: 9, Batch: 25, Avg. Loss: 0.11388549115508795\n",
      "Epoch: 9, Batch: 29, Avg. Loss: 0.15228622313588858\n",
      "Epoch: 9, Batch: 33, Avg. Loss: 0.14217243157327175\n",
      "Epoch: 9, Batch: 37, Avg. Loss: 0.10264252591878176\n",
      "Epoch: 9, Batch: 41, Avg. Loss: 0.11370193772017956\n",
      "Epoch: 9, Batch: 45, Avg. Loss: 0.1434251107275486\n",
      "Epoch: 9, Batch: 49, Avg. Loss: 0.15954015776515007\n",
      "Epoch: 9, Batch: 53, Avg. Loss: 0.17176442965865135\n",
      "Epoch: 9, Batch: 57, Avg. Loss: 0.12991529144346714\n",
      "Epoch: 9, Batch: 61, Avg. Loss: 0.14260252099484205\n",
      "Epoch: 9, Batch: 65, Avg. Loss: 0.1447664014995098\n",
      "Epoch: 9, Batch: 69, Avg. Loss: 0.2011392079293728\n",
      "Epoch: 9, Batch: 73, Avg. Loss: 0.11024301871657372\n",
      "Epoch: 9, Batch: 77, Avg. Loss: 0.11857056152075529\n",
      "Epoch: 9, Batch: 81, Avg. Loss: 0.12156698666512966\n",
      "Epoch: 9, Batch: 85, Avg. Loss: 0.12435503490269184\n",
      "Epoch: 9, Batch: 89, Avg. Loss: 0.14305266365408897\n",
      "Epoch: 9, Batch: 93, Avg. Loss: 0.14568408764898777\n",
      "Epoch: 9, Batch: 97, Avg. Loss: 0.16131087206304073\n",
      "Epoch: 9, Batch: 101, Avg. Loss: 0.12284314259886742\n",
      "Epoch: 9, Batch: 105, Avg. Loss: 0.140507984906435\n",
      "Epoch: 9, Batch: 109, Avg. Loss: 0.14721346832811832\n",
      "Epoch: 9, Batch: 113, Avg. Loss: 0.19381042942404747\n",
      "Epoch: 9, Batch: 117, Avg. Loss: 0.1261877566576004\n",
      "Epoch: 9, Batch: 121, Avg. Loss: 0.09198080189526081\n",
      "Epoch: 9, Batch: 125, Avg. Loss: 0.1313637401908636\n",
      "Epoch: 9, Batch: 129, Avg. Loss: 0.09192827809602022\n",
      "Epoch: 9, Batch: 133, Avg. Loss: 0.06468341499567032\n",
      "Epoch: 9, Batch: 137, Avg. Loss: 0.09599900152534246\n",
      "Epoch: 9, Batch: 141, Avg. Loss: 0.12703058030456305\n",
      "Epoch: 9, Batch: 145, Avg. Loss: 0.1075334819033742\n",
      "Epoch: 9, Batch: 149, Avg. Loss: 0.1534979809075594\n",
      "Epoch: 9, Batch: 153, Avg. Loss: 0.07771921157836914\n",
      "Epoch: 9, Batch: 157, Avg. Loss: 0.12717686966061592\n",
      "Epoch: 9, Batch: 161, Avg. Loss: 0.20246131904423237\n",
      "Epoch: 9, Batch: 165, Avg. Loss: 0.13744470523670316\n",
      "Epoch: 9, Batch: 169, Avg. Loss: 0.1283995658159256\n",
      "Epoch: 9, Batch: 173, Avg. Loss: 0.10107096284627914\n",
      "Epoch: 9, Batch: 177, Avg. Loss: 0.22751299664378166\n",
      "Epoch: 9, Batch: 181, Avg. Loss: 0.11504767648875713\n",
      "Epoch: 9, Batch: 185, Avg. Loss: 0.16639646142721176\n",
      "Epoch: 9, Batch: 189, Avg. Loss: 0.12962769344449043\n",
      "Epoch: 9, Batch: 193, Avg. Loss: 0.11952249146997929\n",
      "Epoch: 9, Batch: 197, Avg. Loss: 0.12264995649456978\n",
      "Epoch: 9, Batch: 201, Avg. Loss: 0.14080841280519962\n",
      "Epoch: 9, Batch: 205, Avg. Loss: 0.14914671517908573\n",
      "Epoch: 9, Batch: 209, Avg. Loss: 0.10470600426197052\n",
      "Epoch: 9, Batch: 213, Avg. Loss: 0.19108639284968376\n",
      "Epoch: 9, Batch: 217, Avg. Loss: 0.21331118047237396\n",
      "Epoch: 9, Batch: 221, Avg. Loss: 0.1786597091704607\n",
      "Epoch: 9, Batch: 225, Avg. Loss: 0.12033172976225615\n",
      "Epoch: 9, Batch: 229, Avg. Loss: 0.18934683874249458\n",
      "Epoch: 9, Batch: 233, Avg. Loss: 0.11683239415287971\n",
      "Epoch: 10, Batch: 1, Avg. Loss: 0.05408952012658119\n",
      "Epoch: 10, Batch: 5, Avg. Loss: 0.2031482197344303\n",
      "Epoch: 10, Batch: 9, Avg. Loss: 0.0940551869571209\n",
      "Epoch: 10, Batch: 13, Avg. Loss: 0.11577947437763214\n",
      "Epoch: 10, Batch: 17, Avg. Loss: 0.16921181418001652\n",
      "Epoch: 10, Batch: 21, Avg. Loss: 0.14311537519097328\n",
      "Epoch: 10, Batch: 25, Avg. Loss: 0.10751720145344734\n",
      "Epoch: 10, Batch: 29, Avg. Loss: 0.0999985346570611\n",
      "Epoch: 10, Batch: 33, Avg. Loss: 0.09469250682741404\n",
      "Epoch: 10, Batch: 37, Avg. Loss: 0.13054785039275885\n",
      "Epoch: 10, Batch: 41, Avg. Loss: 0.12025529611855745\n",
      "Epoch: 10, Batch: 45, Avg. Loss: 0.11293379217386246\n",
      "Epoch: 10, Batch: 49, Avg. Loss: 0.07386625278741121\n",
      "Epoch: 10, Batch: 53, Avg. Loss: 0.17416000179946423\n",
      "Epoch: 10, Batch: 57, Avg. Loss: 0.10734187439084053\n",
      "Epoch: 10, Batch: 61, Avg. Loss: 0.07855683658272028\n",
      "Epoch: 10, Batch: 65, Avg. Loss: 0.1338624767959118\n",
      "Epoch: 10, Batch: 69, Avg. Loss: 0.1748758666217327\n",
      "Epoch: 10, Batch: 73, Avg. Loss: 0.10828064382076263\n",
      "Epoch: 10, Batch: 77, Avg. Loss: 0.09474066458642483\n",
      "Epoch: 10, Batch: 81, Avg. Loss: 0.14339923299849033\n",
      "Epoch: 10, Batch: 85, Avg. Loss: 0.20354461669921875\n",
      "Epoch: 10, Batch: 89, Avg. Loss: 0.11801744066178799\n",
      "Epoch: 10, Batch: 93, Avg. Loss: 0.14912133663892746\n",
      "Epoch: 10, Batch: 97, Avg. Loss: 0.1695791007950902\n",
      "Epoch: 10, Batch: 101, Avg. Loss: 0.14167997799813747\n",
      "Epoch: 10, Batch: 105, Avg. Loss: 0.09415613487362862\n",
      "Epoch: 10, Batch: 109, Avg. Loss: 0.10877889394760132\n",
      "Epoch: 10, Batch: 113, Avg. Loss: 0.16151936817914248\n",
      "Epoch: 10, Batch: 117, Avg. Loss: 0.15438492875546217\n",
      "Epoch: 10, Batch: 121, Avg. Loss: 0.09683527145534754\n",
      "Epoch: 10, Batch: 125, Avg. Loss: 0.17280283942818642\n",
      "Epoch: 10, Batch: 129, Avg. Loss: 0.1385067980736494\n",
      "Epoch: 10, Batch: 133, Avg. Loss: 0.12966938689351082\n",
      "Epoch: 10, Batch: 137, Avg. Loss: 0.12218951247632504\n",
      "Epoch: 10, Batch: 141, Avg. Loss: 0.1253405399620533\n",
      "Epoch: 10, Batch: 145, Avg. Loss: 0.1502330657094717\n",
      "Epoch: 10, Batch: 149, Avg. Loss: 0.1651070062071085\n",
      "Epoch: 10, Batch: 153, Avg. Loss: 0.2364052888005972\n",
      "Epoch: 10, Batch: 157, Avg. Loss: 0.21208053082227707\n",
      "Epoch: 10, Batch: 161, Avg. Loss: 0.12712367251515388\n",
      "Epoch: 10, Batch: 165, Avg. Loss: 0.1373064797371626\n",
      "Epoch: 10, Batch: 169, Avg. Loss: 0.0881050517782569\n",
      "Epoch: 10, Batch: 173, Avg. Loss: 0.14349042437970638\n",
      "Epoch: 10, Batch: 177, Avg. Loss: 0.13727013021707535\n",
      "Epoch: 10, Batch: 181, Avg. Loss: 0.18701145984232426\n",
      "Epoch: 10, Batch: 185, Avg. Loss: 0.07420992199331522\n",
      "Epoch: 10, Batch: 189, Avg. Loss: 0.09197837114334106\n",
      "Epoch: 10, Batch: 193, Avg. Loss: 0.14508706331253052\n",
      "Epoch: 10, Batch: 197, Avg. Loss: 0.10268271155655384\n",
      "Epoch: 10, Batch: 201, Avg. Loss: 0.16419111378490925\n",
      "Epoch: 10, Batch: 205, Avg. Loss: 0.12277097161859274\n",
      "Epoch: 10, Batch: 209, Avg. Loss: 0.07670432515442371\n",
      "Epoch: 10, Batch: 213, Avg. Loss: 0.22075613774359226\n",
      "Epoch: 10, Batch: 217, Avg. Loss: 0.11994730774313211\n",
      "Epoch: 10, Batch: 221, Avg. Loss: 0.07935598213225603\n",
      "Epoch: 10, Batch: 225, Avg. Loss: 0.13938524201512337\n",
      "Epoch: 10, Batch: 229, Avg. Loss: 0.14561951160430908\n",
      "Epoch: 10, Batch: 233, Avg. Loss: 0.2737535536289215\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "print_every = 4\n",
    "\n",
    "loss_over_time = [] # to track the loss as the network trains\n",
    "    \n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_i, (input_data, labels) in enumerate(train_loader):\n",
    "        # Zero gradients (just in case)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass, calculate predictions\n",
    "        output = lstm_model(input_data) \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, labels)\n",
    "        ## Backward propagation\n",
    "        loss.backward()\n",
    "        ## Upade weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print loss statistics\n",
    "        # to convert loss into a scalar and add it to running_loss, we use .item()\n",
    "        running_loss += loss.item()\n",
    "            \n",
    "        \n",
    "        if batch_i % print_every == 0:    # print every 100 batches (staring from 50)\n",
    "                avg_loss = running_loss/print_every\n",
    "                # record and print the avg loss over the 100 batches\n",
    "                loss_over_time.append(avg_loss)\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, avg_loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x135111a90>]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "input_data, labels = dataiter.next()\n",
    "lstm_model(input_data) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.000057\n",
      "\n",
      "Test Accuracy of toxic: 90% (2258/2500)\n",
      "Test Accuracy of severe_toxic: 99% (2477/2500)\n",
      "Test Accuracy of obscene: 94% (2359/2500)\n",
      "Test Accuracy of threat: 99% (2492/2500)\n",
      "Test Accuracy of insult: 94% (2368/2500)\n",
      "Test Accuracy of identity_hate: 99% (2484/2500)\n",
      "\n",
      "Test Accuracy (Overall): 96% (14438/15000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_classes = 6\n",
    "# initialize tensor and lists to monitor test loss and accuracy\n",
    "test_loss = torch.zeros(1)\n",
    "class_correct = list(0. for i in range(num_classes))\n",
    "class_total = list(0. for i in range(num_classes))\n",
    "\n",
    "# set the module to evaluation mode\n",
    "lstm_model.eval()\n",
    "\n",
    "# get the input images and their corresponding labels\n",
    "inputs, labels = test_loader.dataset.tensors\n",
    "\n",
    "# forward pass to get outputs\n",
    "outputs = lstm_model(inputs)\n",
    "\n",
    "# calculate the loss\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "# update average test loss \n",
    "test_loss = test_loss + ((torch.ones(1) / (len(labels) + 1)) * (loss.data - test_loss))\n",
    "\n",
    "# get the predicted class from the maximum value in the output-list of class scores\n",
    "for j in range(num_classes):\n",
    "    # compare predictions to true label\n",
    "    predicted_class = np.round(outputs.data[:,j])\n",
    "    labels_class = labels.data[:,j]\n",
    "    class_correct[j] = (predicted_class == labels_class).sum()\n",
    "    class_total[j] = len(labels)\n",
    "           \n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss.numpy()[0]))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            label_colnames[i], 100 * class_correct[i] / class_total[i],\n",
    "            (class_correct[i]), (class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (label_colnames[i]))\n",
    "\n",
    "        \n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2257.)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(labels)) - labels[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "250-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
